Structured Outputs in Competitive Systems: A Comparative Analysis
Simulation AI Competitions (Halite, Battlecode, RoboCup, etc.)
Halite: The Halite programming challenge (2016–2018) shows a progression in log design. Halite I’s arena logs were essentially sequential text entries capturing setup, turn-by-turn moves, and results. These text logs provided a narrative of each game but required custom parsing. In later iterations (Halite II/III), the competition shifted to structured replay files (with extension .hlt). These replays were JSON-based: for example, one contest described the log file as a JSON array indexed by turn, with each turn containing per-player data. This made data consumption easier for analysis and visualization. Participants could download raw replay files and even run them through a provided web visualizer. A major benefit was enabling machine learning on game data – one competitor noted that “the raw replays were available in JSON”, which allowed parsing games into training tensors. A tradeoff was file size and complexity: storing full grid states each turn in JSON could bloat logs, but it eliminated the need for a custom simulator to interpret moves. Halite maintained trust by making the game deterministic (seeded RNG) and relying on the authoritative server-run; there was no need for cryptographic signatures since all matches ran on the platform. Determinism meant a JSON replay could serve as ground truth to reproduce the outcome exactly.
Battlecode: MIT’s Battlecode (annual competition) takes a slightly different approach. Replays are saved as files (e.g. .bc21 for 2021) that can be opened in a custom viewer. The underlying format is not human-readable JSON; instead, Battlecode logs events in a sequential turn-by-turn record with key actions. For example, the 2025 Battlecode game’s log “begins with setup information…and then each line corresponds to a turn,” tagging which unit acted and what happened. This yields a narrative of the match (e.g. unit X spawned or attacked each turn). Such text-centric logs are readable and replayable with the official client, but parsing them programmatically can be laborious. Indeed, researchers building tooling for multiple games noted that “trajectories are tedious to parse” in raw Battlecode logs, prompting them to write a viewer to extract structured stats. One known issue in Battlecode was log completeness and size: at one point the replay files did not store certain world state (e.g. a “global pollution level” in one version) and grew large, so the developers patched the format to include the missing info and even added an option to limit log size. This highlights a common tradeoff: complete information vs. compact logs. Battlecode opted to log enough to replay the match (and later fixed omissions), but they considered truncating or compressing less critical data to keep file sizes manageable. Trust in Battlecode replays comes from using the official engine to play them back; there’s no evidence of cryptographic verification, but the community relies on the deterministic engine and the fact that replays are produced by the trusted server.
RoboCup Simulation: In RoboCup Soccer simulations, matches are logged in proprietary binary formats, later convertible for analysis. For instance, the 2D Soccer Simulation League produces .rcg log files which capture every simulation cycle. Tools exist to convert these logs into CSV or JSON streams for human analysis. The small-size robot league similarly records logs and even provides a log converter that emits a JSON event stream from the binary file. The data architecture here favors efficient recording (binary for performance) with post hoc conversion to structured data for external consumption. A key consideration was determinism and reproducibility: the RoboCup simulator allows replays of .rcg logs using a log-player utility, ensuring that given the log (plus the simulator), one can exactly reproduce the match. This mirrors a truth-first model: the log is the ground truth of what occurred, and any visual or statistical output is derived after. One challenge faced in the 3D simulation league was that replaying logs could be non-deterministic or problematic if the simulator version differed. This hints at a failure mode: if the platform evolves, older logs might not play back identically – a regret similar to what StarCraft players experienced with version updates (discussed later). RoboCup logs also contain hidden information scenarios (each agent’s private sensor inputs). Typically, the complete log includes all data (for full replay), but viewers might only show public information during a live match. This separation wasn’t always formally defined in older RoboCup tools, but HashMatch’s design explicitly tackles it with redaction policies, learning from such needs.
Google AI Challenge (Ants): The 2011 Google AI Challenge “Ants” game also produced structured match data. The competition site allowed downloading replays in JSON format – in fact, the game spec explicitly stated replayformat could be JSON, and the replaydata field contained the replay in that format. Participants could view games in a browser app or use the raw JSON for analysis. This design decision (using JSON) made it easier for competitors to write their own analysis tools or even custom visualizers. A minor drawback was file size and parsing speed, but given the relatively small grid and turn count, JSON was manageable. Like others, the game engine was deterministic and provided the final authority on outcomes; the JSON replays were implicitly trusted. An interesting pattern here is that the schema was scenario-specific – e.g. the ants replay JSON described ant locations, food, etc. – yet the competition platform had a generic interface to handle different games. This suggests that even if each game had custom turn events, using a structured text format consistently (JSON) was seen as good practice for external consumption. It allowed the presentation layer (visualizer) to be decoupled from the simulation runtime, reading from a data file.
Key takeaways: Simulation contests generally treat the event log as the source of truth and expose it either directly or in a post-processed form. Early or simpler systems (Halite I, Battlecode) favored human-readable sequential logs, which are easy to examine but require effort to parse rigorously. Later systems moved toward structured artifacts (JSON, binary replay files) that enable programmatic consumption and community tool-building. A recurring theme is deterministic reproducibility: by logging every relevant event or using a fixed random seed, these platforms ensure an external user can trust the replay to reflect the true match outcome. Tradeoffs appear around verbosity vs. efficiency – e.g. log complete state each turn (simpler to consume, as Halite did) versus log only incremental actions (smaller files but needing an engine to reconstruct state). The trust model in these contests typically relies on the central server and open replays; explicit cryptographic verification was not common, likely because organizers ran everything centrally. This contrasts with HashMatch’s plan to include hash receipts for verification – a belt-and-suspenders approach that most past competitions didn’t deem necessary (they assumed trust in the organizer). Finally, supporting spectator-friendly output is a growing priority: newer designs separate the raw “truth” log from derived highlights or summaries, so that viewers can be shown exciting moments without ever contradicting the underlying log. This layered approach is implicit in some competitions (where a human broadcasts highlights from a replay), but HashMatch is making it explicit and automated.
Board Game Archives (Chess & Go)
Competitive board games like chess and Go have long-established standards for recording and sharing game results as artifact files. In chess, the universal format is PGN (Portable Game Notation). A PGN file contains structured data: a header with metadata (players, event, result, etc.) and a move list in algebraic notation. Crucially, PGN is human-readable and very lightweight – a full game’s moves are listed in sequence. This means any external consumer (from a casual player to a database or engine) can parse the moves and reconstruct the game’s state move by move using the known chess rules. This design reflects a trust model of transparency and reproducibility: given the move sequence, one can verify the outcome (checkmate, draw, etc.) by applying the standard rules. There is no need for a proprietary viewer; countless tools exist to parse PGNs or display them because the format is public and simple. One benefit of this simplicity is longevity: PGN files from decades ago are still readable today, unaffected by software version changes. A tradeoff is that PGN relies on external knowledge (the chess rules) to interpret – it doesn’t explicitly log the board state, only the moves. But since chess rules are fixed and globally understood, this is an acceptable design. PGN also separates data from presentation: commentary or analysis can be added in comments or a separate annotated file, but the base PGN remains a raw record of moves.
Modern chess platforms like Lichess expose games via APIs in multiple formats, including PGN and JSON. Lichess allows users to download game records as PGN or get them programmatically in JSON form (with moves, timestamps, etc.), supporting a thriving ecosystem of external analysis tools. This openness is considered a success: it encourages community contributions (analytics, visualizers, studies) by providing structured outputs over an API without needing to scrape a website. Other platforms (Chess.com, for example) also offer PGN exports. The only trust concern in online chess data is fair play (cheating), which is outside the scope of the game record format; the PGN itself is assumed honest. Notably, digital signatures or hashes are not used – a PGN could theoretically be edited by anyone, but in practice if it comes from an official source or server it’s accepted. The immutable truth is ensured by the authority of tournament organizers or server logs rather than by cryptographic means.
In Go (Weiqi), the analogous format is SGF (Smart Game Format). It records moves and positions (using coordinate pairs) along with metadata. Like PGN, SGF is text-based and easily shared, and it can also include variations and comments. Online Go servers such as OGS (Online Go Server) or KGS allow downloading games in SGF. These artifact-centric approaches demonstrate a clear pattern: separate the core record of the contest from any specific UI or application. By doing so, the data endures and can be consumed in unforeseen ways. For instance, PGN and SGF archives fuel research (e.g. training AI like AlphaGo was enabled by large SGF databases of professional games). A regret in this domain might be that these formats, being old, weren’t initially designed for modern needs like embedding time-per-move or multimedia. Extensions exist (for example, PGN can include clock times in comments, and SGF has some support for timing), but the core formats are minimal. Still, the benefit of minimalism is broad adoption – any extra data is usually handled by separate files or newer standards (chess has JSON-based formats like Chess Game Format for more detailed info, but PGN remains the lingua franca).
One tradeoff highlighted by board game data is real-time vs archival format. PGN/SGF are typically produced once a game is complete. For live broadcast of moves, other mechanisms are used (like live PGN streams or dedicated protocols such as FEN updates or the Lichess live API). This separation ensures the archival format stays clean and final, while live spectators get updates through a different channel. HashMatch’s design similarly suggests that the truth layer (like a complete match JSONL log) would be finalized after the match, whereas a live broadcast might use the telemetry layer or a filtered feed with no spoilers. Chess has dealt with “spoilers” in the sense that some viewers prefer not to know the final result; websites sometimes hide the result unless requested. But by and large, in board games the concept of hidden information during the game is rare (except in specific variants), so the logs don’t require redaction. HashMatch’s need to redact secret info (like an agent’s private observations) mid-match has parallels in games like poker (where hand histories are revealed only after a hand is over). Poker is another domain where structured logs (hand history files) are exported for later analysis, but kept partially secret in real-time. The success of PGN/SGF underscores the value of standardized, portable artifacts. It’s a design that stood the test of time and enabled rich external consumption – from printing game collections in books to powering today’s AI training sets.
Replay-Centric Esports (StarCraft, Dota, Fighting Games)
In esports, especially complex video games, the primary structured output is often a replay file. A replay is essentially a recording of all actions and events that occurred, distinct from a video recording – it’s data that the game engine can use to reconstruct the match.
StarCraft (Brood War & StarCraft II): StarCraft: Brood War introduced the .rep replay format decades ago, and StarCraft II uses .SC2Replay files. These are binary files containing initial game state, player commands, and random seeds. The data architecture choice here is to log only the essential inputs (actions) and rely on the game’s deterministic engine to reproduce all outcomes. This keeps replay files relatively small (compared to recording every unit’s state every frame) and is efficient for storage. The tradeoff, however, is tight coupling to the game version: whenever Blizzard updated StarCraft II’s game balance or mechanics, older replays often became incompatible (since the engine simulation changed). This is a known failure mode: players lament that after major patches, they cannot open old replays unless they keep an old version of the client. The community partly worked around this by maintaining multiple game installs or using third-party replay parsers that could at least extract some data without the game. The regret here is not having a stable, versioned replay spec – a lesson for HashMatch to consider if the platform will evolve. To mitigate this, HashMatch documentation emphasizes version stamping in manifests (including game, scenario, and engine versions), so that a replay’s provenance is clear and the correct version of the engine can be used.
For external consumption, StarCraft replays required either the game client or custom tools. Over time, enthusiasts built parsers to convert replay files into JSON or databases for analysis (e.g. extracting build orders, actions per minute, etc.). Notably, Blizzard did not provide an official open API for match records; the community shouldered that via replay parsing. This contrasts with some newer esports.
Dota 2: Valve’s Dota 2 (a popular MOBA) initially provided a Web API for match data. Using an HTTP request with a match ID, one could retrieve a JSON payload of detailed stats: hero picks, kills, item purchases, timestamps of objectives, and so on. This API approach allowed many third-party services (like DotaBuff, OpenDota) to flourish without parsing replay files themselves – the structured output was readily available. However, Valve’s support for this API has been inconsistent. In 2016, a prolonged outage of the Dota 2 match API caused significant problems for apps dependent on it. Later, Valve introduced tighter privacy, requiring players to opt in to data sharing, which meant many matches became inaccessible via the API. Eventually, community-run services like OpenDota started downloading the raw replay files (which Valve still provided for each match) and parsing them to get around API limitations. The lesson is that relying on a live API can be risky if the provider changes policy or infrastructure – a risk HashMatch should note if it ever exposes a live data API. Some in the Dota community regretted the API changes, as it “proved problematic for web apps” and leagues that depended on automating data fetching.
On the other hand, Dota’s replay files (and similarly League of Legends’s replay logs) are rich sources of truth. The trust model is that these files come from the game servers and can be replayed in the client, so they are authoritative. They are not user-editable (often encrypted or signed in subtle ways), which prevents tampering – a necessary safeguard given the competitive stakes. One interesting design decision: Dota 2’s replay and API data exclude fog-of-war information unless you have certain spectating rights. This means if you fetch data as a third-party, you might only see what an unbiased observer would, not the internal team perspectives. This is akin to HashMatch’s planned modes where certain information is redacted for spectators. It helps prevent cheating (e.g., in a live context, you can’t get enemy vision info from the API). After matches, however, full data can be revealed.
Fighting games (e.g. Street Fighter, Super Smash Bros): Historically, fighting games have not provided robust structured logs to the public. Replays exist (often as video or as inputs recorded by the game), but external consumption is usually limited to watching video highlights. There have been fan projects where players extract frame-by-frame data or input sequences, but official APIs are rare. This highlights a counter-example: lack of structured output can limit the community’s analytical engagement. Players resort to crowd-sourced data gathering (for combo statistics, character win rates) by parsing tournament videos or using mods. The tradeoff here was likely ease of development for game studios vs. community needs – it wasn’t a priority to output match data because fighting game communities historically thrived on manual analysis and the excitement of live viewing. However, as esports analytics grows, we see emerging tools to log hits, damage, etc., in community-run events. For HashMatch, this is a caution that if you don’t provide data, the community will either lose insight or hack around it. Given HashMatch’s “spectator-first” goal, providing rich structured outputs (like complete event logs and statistics) is almost a requirement to enable commentary, analysis, and fan content.
Recurring patterns in esports replays include: post-match data derivation – e.g., StarCraft doesn’t store a graph of resources over time explicitly, but any replay viewer can compute it by simulating the replay. Similarly, Dota’s API provided net worth over time which is derivable from events. Many systems choose not to store redundant telemetry in the replay to keep it minimal, trusting that external tools or later computations can derive it. HashMatch mirrors this: telemetry like score timelines or “moments” can be computed from the base event log. The risk is if the computation is expensive or complex, some systems eventually decided to store it anyway for convenience (e.g., Valve’s API gives final stats directly to avoid each client recomputing). HashMatch actually hybridizes this: it plans to output optional match_summary.json and moments.json (highlights) for convenience, while still insisting they can be recomputed if needed. Another pattern is compatibility and versioning: esports games show that without careful version tracking, external consumption suffers. HashMatch’s use of manifests and version hashes is an attempt to avoid the StarCraft replay problem by explicitly tying a match to the exact code versions used – something real-world systems like ICPC and sports have also done (as we’ll see next).
Traditional Sports Data Platforms (Opta, Sportradar, ESPN)
Professional sports competitions produce enormous amounts of structured data for consumption by broadcasters, apps, and bettors. Unlike the self-contained digital competitions above, sports data is often collected by human annotators or tracking systems and distributed via live data feeds.
Opta Sports (StatsPerform): Opta is a leading provider of football (soccer) data. Opta’s system involves live analysts recording every event (pass, shot, tackle, etc.) according to a strict taxonomy. These events are fed in real-time to clients (typically as XML or JSON messages over an API). Opta has standardized feed formats – for example, F24 is a detailed event feed in JSON that companies can subscribe to. The data architecture is typically hierarchical: a match feed will contain a list of events, each event with attributes like timestamp, player, x/y coordinates, and a code for event type (Opta maintains definitions for each code to ensure consistency). They also provide summary feeds (like F9 for line-ups and stats). This separation of granular events from summary mirrors the truth/telemetry split: raw events are truth (what happened on the field), and summary stats (possession percentage, shots count) are derived telemetry. In practice, Opta often distributes both – so consumers who just want high-level stats can use the summary feed, while those who need every detail (e.g., for advanced analytics) ingest the full event feed.
A trust model challenge for sports data is accuracy. Since data may be entered by humans in real-time, errors occur. Opta has processes to correct data after review, producing updated feeds post-match. However, this leads to tension for external consumers. For example, in betting, a bet might be settled on the initial data (e.g., a shot on target count) and then Opta corrects an event (removing a shot) after 15 minutes, causing bets to be retrospectively invalidated. Bettors have found themselves at the mercy of Opta’s data, with no recourse even if video evidence shows the data was initially wrong. This highlights a failure mode in external consumption: when the external system (bookmakers, fantasy apps) treats the data feed as gospel truth, any errors or late changes can propagate into user-facing “facts” that conflict with reality. Bookmakers generally write into terms that “official data is final”, placing trust entirely on the provider. The regret is that there’s “no effective way to challenge it” when the data is wrong – a stark contrast to e.g. HashMatch where one could imagine verifying a match’s log if there’s a dispute. The sports world is grappling with this: some advocate for more transparency or even second-by-second verification (perhaps via video-assisted review) for data that decides bets. The key pattern is that trust is offloaded to a central authority (the data provider), and the external consumers must abide, even if errors occur. This is why HashMatch’s notion of cryptographically signed truth logs and receipts is interesting; it could allow a kind of verification and audit trail that in sports is currently opaque.
Sportradar and ESPN APIs: Sportradar is another major provider that offers Web APIs for many sports. Clients (like sports apps) use API keys to fetch schedules, live scores, play-by-play events, etc. These are usually in JSON and updated frequently. One positive of this approach is that it cleanly separates presentation from data – e.g., ESPN’s apps or websites can call the same API that external partners do to get scores and statistics, then render it in their own style. However, not all providers keep their APIs open. Notably, ESPN itself shut down its public API around 2014, despite previously allowing developers access to scores and stats. They likely found that an open API didn’t directly benefit them and potentially caused maintenance burden or data misuse. This decision was widely lamented by developers (who then resorted to scraping ESPN’s site or finding unofficial endpoints). The lesson here is that data platforms weigh openness vs. control. Codeforces or Lichess chose openness to empower the community; ESPN chose control to drive users to its own platforms. For HashMatch, which position to take is an open question: a fully open consumption API could spur community tools, but it might reduce traffic to official channels or raise security concerns with third-party betting or analytics. The middle ground could be providing artifacts or data dumps (like tournament JSON bundles) after events, rather than a continuously queryable public API. This resembles what the ICPC programming contest does (discussed next).
Another pattern in sports data is the layered approach to truth vs narrative. The raw play-by-play feed (e.g., “Player X scored at minute 10”) is akin to truth data. On top of that, broadcasters add a narrative layer: commentary, graphics (“heat maps”, “win probability graphs”), and so forth. This is done externally by consuming the data feed. The data feeds themselves sometimes start venturing into the narrative domain – for example, some providers now supply expected goals (xG) statistics or win probability models in real-time. These are derived metrics, not ground truth, yet they are fed alongside the events. If HashMatch aims to incorporate such metrics (like a “watchability score” or AI-generated insights), it faces the same choice: do you include these in the official data output (telemetry layer), or only generate them client-side? Sports data providers have found value in including derived analytics in their feeds to enhance storytelling (e.g., “team A has a 5% chance to come back now”). The tradeoff is that including derived data can lock in certain definitions or models (Opta’s xG model might differ from another – consumers then either trust Opta’s or compute their own). HashMatch’s plan is to compute some derived files (moments, watchability) but label them non-authoritative, which follows the best practice of never mixing up what actually happened with how you interpret it.
Finally, in sports, data packaging for external use is evolving. The ICPC case below actually parallels what sports have done with “contest packages.”
Competitive Programming Contests (Codeforces, ICPC)
Competitive programming platforms also expose structured outputs, primarily for standings and submissions.
Codeforces: Codeforces provides a public read-only HTTP API that returns contest data in JSON format. External developers can fetch contest standings, problem results, and even user submission details. This API has enabled community-made apps (like virtual contest trackers, rating predictors, etc.). Because programming contests are less about real-time play-by-play and more about discrete outcomes (who solved which problem when), the data model is simpler: essentially a list of problems, a list of submissions with timestamps and verdicts, and a ranking formula. Codeforces’ design choice was to expose raw data (submissions, test results) but not any narrative – the commentary or editorial insights are separate (as human-written editorials). A notable incident was Codeforces dealing with scoreboard integrity: on rare occasions, cheating or plagiarism is discovered after a contest, and they adjust the standings. They do this by re-scoring or disqualifying entries, and the API then reflects updated standings. This is analogous to sports data corrections, though far less frequent. The trust model is straightforward: Codeforces is the central authority, and its API is the ground truth of contest results. No signatures are needed because if one distrusts the data, one implicitly distrusts the platform itself. One can see every submission’s details (source code, output) on the site for verification if needed.
ICPC (International Collegiate Programming Contest): ICPC has multiple independent contest systems (PC^2, Kattis, etc.), so they introduced a unifying Contest API spec to standardize how contest data is exposed. This spec includes endpoints for teams, problems, submissions, and a scoreboard feed. A key innovation is the Contest Package – essentially a snapshot of the entire contest data (teams, problems, final standings, and all submissions/judgements) in a single archive file. This package is a portable artifact that can be used to reproduce or analyze the contest after the fact. For example, the ICPC World Finals provide a contest package after the event, allowing analysts to run a “replay” of the contest in a resolver tool that graphically shows the progression of scores. This parallels HashMatch’s tournament bundle concept, which packages all match results and even match bundles into a folder for offline sharing. The integrity and provenance are crucial here: the package often contains a manifest with checksums and contest metadata so that one can verify they have the exact official data. ICPC’s motivation was similar to HashMatch’s goals of portability and verifiability – multiple regional contests can be audited or merged if they adhere to the spec.
One tradeoff with the contest API approach is complexity: it had to accommodate different contest formats and live updates (ICPC contests often have a “frozen” period where the scoreboard is hidden from contestants near the end). The spec thus deals with visibility rules, much like HashMatch’s spectator vs admin views. A documented issue in early ICPC was that each system had its own output format (some XML, some plain text), making it hard to develop universal spectator tools. The unified JSON API and package format resolved that, but only after years of iteration.
Failure modes and regrets: A noteworthy one in programming contests occurred in older systems: manual transcription or PDF outputs of results led to errors. For instance, before unified digital feeds, a wrong rank or score could be announced and later corrected, causing confusion. The push to structured data feeds (and backing them with logs of every submission) was to ensure that what you see on the scoreboard is derived from an auditable list of judgements. This is analogous to sports ensuring a scoreboard is backed by play-by-play data. We see again the pattern of truth (submission log) feeding telemetry (scoreboard). When these get out of sync (say a submission is rejudged but the scoreboard not updated), it’s a serious integrity issue. Modern contest software tries to minimize that by real-time linking of events and standings.
ICPC’s contest package also contains the problems and test data, which is interesting for external consumption – it means years later, one can not only see who solved what, but also attempt the problems. HashMatch might not have an exact parallel (since “problems” in HashMatch are game scenarios and code), but the idea of packaging everything needed to reproduce the competition (agents, environment version, logs) is similar.
One unresolved tension in competitive programming data is privacy: should submissions (code) be published? ICPC and Codeforces generally do publish contestants’ code after contests (or at least make it accessible). This is great for learning (external consumers like other contestants or researchers can study strategies) but raises intellectual property questions. HashMatch will face a form of this: will agent code or training data be made public? The external consumption in HashMatch might extend beyond match logs to agent artifacts, if the platform wants a vibrant learning community. That’s an open question – many AI competitions do not open-source participants’ bots without permission.
In summary, competitive programming platforms favor structured JSON outputs for standings, an optional archival package for full data, and they treat the event log (submissions and results) as the ground truth that is always preserved. They have dealt with issues of format standardization and partial information (scoreboard freezing, analogous to HashMatch’s hidden info modes). Their experiences reinforce the value of data consistency and standard formats when multiple parties (teams, coaches, spectators, tool-builders) are consuming the results.
Recurring Patterns and Tradeoffs in External Output Designs

Separation of Layers (Truth vs Derived vs Narrative): Nearly every domain separates the core record of the contest from commentary or fluff. Chess has raw moves (truth) separate from commentary text; sports have play-by-play data separate from broadcast graphics; HashMatch formalizes this into truth/telemetry/show layers. The benefit is integrity (you can regenerate derived stats or change presentation without altering the ground truth). The tradeoff is complexity – ensuring consistency across layers. Real systems that blurred these lines sometimes regretted it (e.g., if a broadcast overlay showed a stat that didn’t match the official record due to a calculation error). Thus a pattern is emerging to keep layers distinct and derive lower layers from higher ones, never the reverse.

Unified Formats vs Game-Specific Formats: There’s a tension between using one generic structure for all scenarios versus tailoring it. Google’s AI Challenge allowed different replay format per game (they specified a format field in the data) but defaulted to JSON for ease. ICPC unified different contest systems under one JSON spec. HashMatch leans toward a single JSONL event log schema for all games. The advantage of a unified schema is that common tools (viewers, validators) work across games. The disadvantage is potential naivety or overhead: a generic format might not capture a game’s nuance as elegantly as a bespoke one. For instance, a generic “event” might need many optional fields to cover all game types, or games might squeeze info into a generic structure in non-intuitive ways. The tradeoff here is between standardization and expressiveness. Most successful systems standardized core fields (time, actor, action, etc.) but allowed extension for game-specific details. HashMatch’s spec likely will evolve similarly, especially as new scenarios bring new event types.

Deterministic Replays vs Complete State Logs: Some systems (StarCraft, Battlecode) record only actions plus a random seed, relying on deterministic simulation to reproduce the match. Others (Halite, many turn-based games) log the complete state or results of each turn in the file. The deterministic minimal-log approach yields smaller files and enforces that one must use the official engine to replay (which can be good for control, e.g., to prevent tampered replays). The complete log approach makes the replay self-contained and easier to parse without an engine, at the cost of size and duplication of simulation logic. The tradeoff is performance vs portability. Notably, HashMatch leans toward logging events (which implies determinism is required to trust that log) but also storing some derived summaries for convenience. As a hybrid, they try to get the best of both worlds. Real-world regrets show that if determinism is not absolute (e.g., physics), minimal logs can fail (replay might not play out the same), which is a serious failure mode. So a system must either guarantee determinism or store enough to avoid ambiguity.

API Access vs Artifact Distribution: External consumers can get data via live APIs (pulling JSON from a web service) or via artifacts (files like PGNs, contest packages, replay archives). APIs are great for real-time and on-demand queries (Codeforces API gives latest standings; Dota API gave match stats instantly). Artifacts are better for archival, bulk analysis, and offline use (a PGN database, an ICPC contest package, a HashMatch broadcast ZIP). The recurring pattern is actually to offer both when possible: an API for immediate needs and artifact dumps for long-term and power-user needs. The tradeoff is maintenance and openness – APIs require running servers and versioning them, and can be shut off (as ESPN did) or suffer downtime, whereas files can be shared without ongoing infrastructure. HashMatch’s current design emphasizes artifact bundles (“no servers required” for replaying data), likely to avoid early complexity. But as the platform grows, it may introduce APIs for convenience (e.g., “get the latest match results as JSON” rather than requiring download of a whole bundle). It’s a strategic decision of community engagement vs. control.

Real-Time Data vs Post-Match Data: Some competitions share data in real-time (live score feeds, live move broadcasts), whereas others prefer to wait until an outcome is decided to release the full data (to prevent exploitation or spoilers). We see this in HashMatch’s mode profiles: a sanctioned mode might hide certain info until the end. Sports often share everything live because spectators demand it, but in betting contexts, sometimes certain detailed stats feed (like player tracking data) might be slightly delayed to avoid abuse. The pattern is to decide what needs to be live and what can be delayed. The tension is between engagement (more live data = more excitement and third-party commentary) and fairness/security (live data might enable cheating or undue advantages if not handled). Each system draws the line differently. Competitive programming hides the scoreboard in the last hour (engagement vs suspense tradeoff). Chess broadcasts often delay by a few minutes in top events to prevent cheating via engine assistance. HashMatch will likely need to fine-tune this for each game scenario (e.g., a poker-like game might share public actions live but hole cards only after).

Data Corrections and Versioning: A recurring challenge is how to handle updates to data. Opta corrects stats post-match, ICPC might adjust results after plagiarism checks, Codeforces recalculates ratings after fixes. The pattern that works is versioning the outputs (HashMatch includes a contract version and tool versions in manifests) and providing a way to communicate updates (like a new contest package version or a new “official replay” if a match is replayed). A common regret is when systems had no mechanism for updates – e.g., early days ESPN published a box score and if an error was found later, there was no clear audit trail or push of corrections, confusing downstream consumers. Modern APIs will send revised data with an indicator of update. HashMatch might consider something akin to a revision or “verdict changed” event if ever a match outcome is voided or corrected (hopefully rare, but bugs happen). In essence, a robust external consumption model must handle the lifecycle of data: creation, updates, and deprecation of stale data.

Community and Openness vs Security: Many patterns boil down to who you empower. Lichess and Codeforces chose to empower the community with data, and they gained rich ecosystems of tools. Valve and ESPN pulled back access, keeping data for themselves, which gave them control but arguably stifled some community innovation. The tradeoff is also security: open data can be abused (e.g., data scraping or cheating – fantasy sports players collating data for advantage, or bots using real-time feeds to cheat in games). HashMatch, dealing with autonomous agents and potentially money prizes, must be cautious. An open API might allow someone to, say, quickly gather all opponents’ past match logs and train counter-strategies – which might be fine (that’s just preparation), but if real-time data of ongoing matches were exposed, an agent could feed it into a live model (if that were allowed) – essentially real-time coaching. So the design has to consider rate limits, access controls, or delayed data to balance openness and fair play.

Documented Failure Modes and Regrets

Incompatible or Ephemeral Replays: StarCraft II’s replay incompatibilities after patches are a classic failure mode. A match record that expires with software updates undermines its value as a long-term artifact. Blizzard likely regrets not baking game rules into the replay or providing a compatibility layer. This taught others (like ICPC and HashMatch) to include version info or ensure a stable playback environment. Another example is if a competition doesn’t log enough info – Battlecode once didn’t log a crucial state (pollution level), meaning replays were incomplete and could lead to divergence if replayed. The fix in code was a lesson: always log all determinism-critical data.

API Withdrawal and Downtime: ESPN shutting down its public API is an oft-cited regret from the developer community perspective. It stranded apps that had integrated it, causing loss of functionality and goodwill. Similarly, Valve’s unstable support for the Dota 2 API (downtime in 2016, later privacy restrictions) disrupted dependent services. The broader lesson: if you offer an external API and later retract or break it, you will burn your ecosystem. Many systems now label their APIs “beta” or unofficial to warn developers. HashMatch can learn that if it promises data access and then changes the contract, it could harm trust. Having a well-defined external consumption contract (like the missing external_consumption_contract_v0.md) upfront can set expectations and reduce this risk.

Misleading Data and Lack of Corrections: Opta’s high-profile mistakes (e.g., missing a valid shot on target in the World Cup) and the inability for consumers to challenge them are regrets in the sports betting world. The failure isn’t that errors happened (humans err), but that the system wasn’t built for transparent correction or challenge. The data consumers (bettors) felt blind and helpless. This indicates that when data is consumed outside, you should consider an error correction mechanism or audit trail. Some proposals include publishing revision histories or confidence levels. No such system is widespread yet in sports, but the clamor is there. For HashMatch, while matches are automated and likely less error-prone in logging, it should still plan for how to handle disputes – e.g., if a bug is found that affected a match outcome, will there be a “strike-through” on that result and an updated result? History from sports says transparency here is key to maintain trust.

Security Breaches and Cheating via Outputs: Thus far, we’ve not encountered a famous case of output data being used maliciously in these examples (more often cheating happens at input or during play). However, one could imagine scenarios: in online chess, if live PGN relays were not delayed, a remote accomplice could feed moves into a chess engine to assist a player (hence top events now delay relays by a move or two). In programming contests, a leaked scoreboard or problem before contest end would ruin the competition – hence the scoreboard freeze mechanism to prevent information leakage. These are incidents averted rather than regrets, reinforcing why such precautions exist. A tangential regret in AI competitions: sometimes organizers did not anticipate how participants would use output logs. For example, if an AI competition published all match replays during the contest, savvy participants might data-mine those replays to improve their bots (an inequality if not everyone does it). Some past contests (possibly Halite or Kaggle competitions) limited information during the competition to prevent an arms race in “meta-learning” from results. The general point: timing and completeness of released data can affect fairness.

Overengineering / Low Adoption Features: It’s worth noting when a system built an elaborate output mechanism that proved unnecessary. One possible example: a competition might design a rich XML schema for match data, only to find that the community never uses the XML and instead relies on simpler summary stats or a provided viewer. If HashMatch over-engineers, say, the “watchability_report.json” or an elaborate commentary template system, but external users just ignore it and focus on the core event log, that extra complexity is a maintenance burden with little benefit. While we don’t have a public postmortem of such from, say, Google AI Challenge or others, it’s easy to imagine. The regret of building features too early is common in software. HashMatch’s docs explicitly say not to lock in decisions prematurely, showing awareness of this risk. A concrete minor example: Lichess once tried a feature to annotate games with engine evaluations in PGNs; it complicated the format and was later dropped in favor of a separate engine analysis feature.

Lack of Standard in Early Days: Before standards like PGN or the ICPC Contest API, data exchange was painful. Early computer chess programmers each used their own notation in the 1970s until PGN in 1994 unified it – prior gamescores sometimes got lost to history or required manual conversion. Similarly, ICPC before 2010s had each regional contest announce winners in whatever format (web pages, PDFs), making aggregate statistics hard. Those communities somewhat regret not standardizing sooner. The creation of standards was itself a response to that pain. So a subtle hindsight lesson: establishing a standard consumption format early can pay dividends, but it has to be flexible enough to last. HashMatch creating a v0 “external consumption contract” shows an attempt to start on the right foot, though it will likely iterate.

Open Questions and Unresolved Tensions
Despite the lessons from existing systems, some design questions remain genuinely tough:

How to balance openness with competitive secrecy? If HashMatch exposes every match log publicly, competitors can study each other’s agents deeply. This is great for learning and transparency, but in a prize-driven competition, teams might desire secrecy (think of it like releasing your playbook). Traditional sports handle this via game film – everyone can watch past games, it’s part of competition. Human esports teams study opponents’ replays extensively. So likely openness will prevail. But there’s tension if any agent logic could be inferred from logs. One unresolved question: should some data (like an agent’s internal state changes, if logged) remain private indefinitely? HashMatch’s current plan logs only environment events, not an agent’s internal decision process, so that seems fine. It errs on the side of openness of outcomes, similar to how chess publishes all moves but not a player’s thoughts.

Will we need cryptographic verification in practice? HashMatch is building in hashing and signing of results, assuming a world with potential distrust (especially with money prizes or decentralized operation). No mainstream competition yet uses blockchain or public verification for match logs – they’ve relied on central authority. It’s unclear if external consumers (spectators, bettors, etc.) actually demand independently verifiable proof of fairness, or if they’re content trusting the organizer. The question is if HashMatch’s audience will care about verifying hashes on match.jsonl, or if that feature remains niche. It might only become vital if HashMatch becomes decentralized or if third parties (e.g., a betting site) want to independently validate that a match wasn’t altered. This tension between maximum trust-through-transparency and practical trust in the organizer is unresolved. It’s a forward-thinking feature that could differentiate HashMatch if one day no central server is trusted, but it might be overkill initially.

How to measure and output “entertainment” or quality of play? HashMatch is unique in aiming to output things like “moments” and “watchability scores”. This veers into narrative and subjective territory. Real systems like Opta or the NBA have tried similar concepts (e.g., identifying “clutch moments” or generating highlight reels automatically). It’s challenging to do well – too many false positives or missing truly interesting context. The open question is how much of this should be automated and part of the official output. If HashMatch includes an official highlights.json, does it risk missing something a human observer would consider the key moment? And if external consumers rely on it, could it shape the narrative in a suboptimal way? The tension is between data-driven storytelling vs. human storytelling. Perhaps the resolution is to output the raw signals (moments, stats) and let human commentators or community content creators interpret them. This is essentially what happens in sports: data providers flag interesting stats, but humans decide the story. HashMatch will likely do the same, but it’s experimenting by including a commentary template system in the show layer (in the future). Whether that succeeds or ends up scrapped is an open design question.

Ensuring longevity of data and tools: We’ve seen how older competition data can become unreadable without old software. An unresolved tension for any evolving platform is how to keep outputs accessible in 5, 10, 50 years. Chess solved this with a plaintext standard. Modern platforms might solve it by open-sourcing the replay viewers or providing format converters. HashMatch as a product might not prioritize 50-year archival, but since it aspires to be a “league”, historical records are part of the appeal. So the question is: will the JSONL and manifest schema remain backwards-compatible? If not, will HashMatch provide migration tools or freeze old versions? The tension between innovation and backward compatibility is constant. Perhaps using JSON (self-describing, extensible) and version tags is sufficient, but it requires discipline to never break the contract badly.

Data ownership and privacy: In competitive programming, contestants’ code becomes public – some have raised mild concerns, but it’s accepted. In AI competitions, if someone uses a proprietary model, would its behavior (implied via outputs) be considered sensitive? Probably not – by competing, you implicitly accept logs are public. However, what if HashMatch logs included something like an agent’s “memory” or private state for debugging? That would reveal internals of someone’s design. Right now, private observations in logs are considered secret only until match end, then presumably they can be revealed. If an agent has a hidden strategy encoded that only shows up in logs, others could reverse-engineer or exploit it next time. The tension here is akin to open-source vs closed-source competition: does revealing everything harm competitive edge or simply drive progress? Most evidence from programming contests and open-source esports bots suggests openness drives progress and people adapt. It remains an open cultural question for HashMatch’s community.

Scaling and automation: As HashMatch grows, generating and serving all these artifacts could become heavy. Imagine hundreds of matches per day, each producing a multi-file bundle. Will they automate bundling and perhaps provide a centralized “replay repository” or API anyway? The current design assumes manual or small-scale sharing (drag-drop a folder to a viewer). The tension will be when volume increases: do they stick to decentralized distribution (everyone downloads what they want) or centralize (a website with an index of all replays, stats searchable)? It’s unresolved how to best deliver potentially thousands of JSONL files to users without a proper service. Solutions might include a registry or database (hinted as future work). This touches on another question: who are the external consumers primarily – casual fans who want a slick interface (which suggests central web portal), or data geeks who will happily fetch raw files (which just need accessible storage)? Likely both exist, and catering to both is challenging without doubling effort.

Handling multi-agent/team scenarios and persistent data: So far, we considered mostly 1v1 matches. But HashMatch envisions team formats, brackets, seasons. External output for a tournament (like a bracket or league standings) adds another layer of structure. The open question is how best to represent and distribute that. ICPC’s contest package handles a single contest’s scoreboard well, but a league season with trades, roster changes, etc., becomes more like traditional sports data. If HashMatch succeeds, one day it might need an API for things like agent profiles, team stats across matches, rankings, etc. Right now, agent_profiles.json is planned as a show artifact, but keeping profile data updated (e.g., win/loss record) is not in scope of a single match bundle. The tension is between focusing on atomic match outputs versus providing aggregated data over time. Many systems handle this by separate outputs: match data vs. an external ratings system (like ELO ratings) updated periodically. It remains to be seen how HashMatch will handle longitudinal data consumption.

What assumptions should we be most suspicious of?
HashMatch’s current design makes several forward-looking assumptions that real-world precedents suggest deserve healthy skepticism:

“Logs are truth and solve disputes” – The assumption that a complete JSONL log is an immutable source of truth should be viewed with caution. In theory, a log plus deterministic code means no ambiguity. In practice, if there’s a bug in the game engine or logging (say an event was recorded incorrectly), the “truth log” could be at odds with what actually happened or should have happened. Other systems have encountered cases where the log was incomplete or needed post-hoc correction (as with Opta or Battlecode). We should be suspicious of the idea that simply having a log guarantees fairness. A log is only as truthful as the system that generated it. HashMatch might need mechanisms for validating and possibly correcting logs (which it hints at with a validation report artifact). In short, trust but verify – even the “truth layer” may need auditing tools beyond just hashing.

Determinism and Reproducibility – HashMatch assumes all matches can be made deterministic via seeding and that anyone can replay a match offline for verification. This is a great ideal, but we should question it for all scenarios. If future scenarios involve any stochastic physics, external APIs, or learning-in-the-loop, true determinism might break. RoboCup’s 3D simulator issues and certain reinforcement learning environments show that minor differences in hardware or timing can break determinism. So being too confident in determinism could be naive. HashMatch might eventually allow non-deterministic modes (exhibition matches, perhaps). In those, the notion of an indisputable replay log falters. Thus, assuming 100% reproducibility is something to be cautious about – it may hold for now (turn-based games, small sims) but not universally.

The need for cryptographic receipts – While a noble security feature, we should ask: is anyone actually going to forge match results, and will anyone independently verify signatures? No current competition does this, and yet trust scandals are extremely rare. It may be overengineering. The assumption to question is that provable integrity is a must-have for a successful competitive platform. It might be if HashMatch involves decentralized execution or user-hosted matches. But if it’s centrally run, players might inherently trust the organizer like they do Codeforces or FIDE. Over-investing in cryptographic proofs could be premature optimization if the user base doesn’t demand it. It’s wise to design for it (as they have), but we should watch carefully whether this feature sees real use or just adds complexity. A false sense of security (“we have receipts, so everything is fair”) is also a risk – it covers only tampering, not issues like biased matchmaking or subtle bugs. So we should be suspicious of any complacency that receipts alone ensure “Trust is mandatory” objectives.

Layered output solves presentation needs – HashMatch splits truth, telemetry, and show data, assuming this will cleanly serve spectators and admins. The suspicion here is that reality might be messier. In practice, some data doesn’t fit neatly in one layer. For example, an admin might want to see a “moment” highlight (which HashMatch calls show layer) to decide a ruling, or a spectator might benefit from a bit of truth data (like a raw log snippet explaining a weird event). Rigid separation could prove either too limiting or be violated often. Other systems have blurred lines when needed – e.g., live sports broadcasts (show layer) now incorporate advanced stats (telemetry) like xG as part of the presentation because it turns out fans appreciate it. HashMatch’s assumption that spectators consume no truth data at all might be too strict. They may find that some raw data (like an event timeline) is actually fine for spectators and enhances transparency. Thus, we shouldn’t treat the current layering as infallible; it will likely evolve with user feedback.

Automated highlights and commentary – The vision of algorithmically generating highlights (moments.json) and even AI commentary (commentary.json) as part of the product is ambitious. We should be skeptical of how quickly or well this can be pulled off. Real-world systems have tried automatic highlight detection with mixed success – many sports still rely on human editors to pick truly meaningful moments from data. AI commentary is even less mature (no major sports broadcast relies on fully automated commentary for good reason). HashMatch’s documents wisely label commentary as “TBD” and moments as an MVP heuristic. The assumption that these can be derived purely from logs is suspect; context and narrative often require more. For instance, a comeback “moment” might be detected by a score swing, but if the audience didn’t know an agent’s backstory or tendency, they might not care. We should be watchful that HashMatch doesn’t over-promise on making matches “prime-time” simply via data outputs. Human involvement or at least iterative tuning will be needed. In short, be suspicious of any fully-automated entertainment claims in the early stages – the data can support it, but not replace the storytelling entirely.

“No servers required” decentralization – The idea that everything can be packaged and shared without a central service is empowering, but perhaps naive in the long run. It assumes users are willing and able to handle files for things like viewing matches or compiling stats. As the user base grows beyond tech-savvy participants to general spectators, relying solely on file bundles might hinder adoption. Other platforms eventually built web portals or APIs on top of their data for accessibility (e.g., the ICPC tools can serve the contest API over HTTP even if the data came from a package). HashMatch’s current assumption seems to be “we can defer building infrastructure by making data portable.” That’s fine for now, but should be viewed as a temporary state. We should question whether a purely client-side/offline approach will hold when live “fight night” events happen – likely, a central broadcast server will still provide the realtime feed (even if it’s composed from truth data). So we shouldn’t take “no servers” too literally; rather, it’s a design principle to not require server-side logic to interpret data. The suspicious angle is if anyone believed HashMatch could scale purely peer-to-peer – in practice, some centralization (even if just for distribution) tends to re-emerge because it’s convenient.

Community will build on the data if we provide it – This is generally true (we’ve seen it with open APIs and replays), but not automatic. Sometimes communities need encouragement or specific support. HashMatch assumes that by exposing structured outputs, third parties will create analyzers, betting markets, training tools, etc. We might be suspicious of an “if you build it, they will come” assumption. For niche competitions, the community might be too small initially to generate a lot of third-party content. So HashMatch may need to build more first-party tools than they expect, until growth reaches a tipping point. For example, they have their own replay viewer now because they couldn’t just wait for the community. It’s a sound strategy, but the general point is to temper expectations of the “ecosystem” until the user base is proven. Many a platform has opened data and seen very few take advantage, because the core product didn’t attract enough attention. Thus, it’s wise to provide data, but also to plan to demonstrate its value (perhaps by showcasing interesting stats or building small apps themselves).

In conclusion, HashMatch’s design is impressively informed by prior art, but by examining where real systems stumbled, we remain duly cautious. Deterministic truth logs, verification receipts, layered outputs, and automated storytelling are all powerful ideas – we just know from history that the devil is in the details. It will be important to continuously validate these assumptions against reality: Are the logs truly capturing everything needed? Do the consumers use the data as expected? Are we solving problems users actually have? By staying suspicious of even our own best assumptions, we ensure the architecture remains grounded and adaptable rather than rigid or prematurely optimized. As the saying goes, “no battle plan survives contact with the enemy” – here, the “enemy” is real-world usage, and HashMatch must be ready to iterate when assumptions meet practice.Sources
