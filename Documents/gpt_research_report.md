Executive Summary

In adversarial AI competitions, procedural content generation (PCG) pipelines must balance variety with fairness and competitive integrity. We propose a four-stage deterministic generation pipeline (structure → fill → validate → score/select) secured by cryptographic seeding and rigorous validation. At each stage, we identify algorithm families (from search-based to grammar-based to ML-driven) and assess their determinism, computational cost, composability in a pipeline, failure modes, and security implications. For example, constructive algorithms like binary space partitioning (BSP) quickly carve out symmetrical dungeon layouts with guaranteed non-overlap, while search-based methods (e.g. evolutionary or reinforcement learning generators) can fine-tune balance at higher cost. Each approach has trade-offs: failure modes like unsolvable outputs (which validators must catch) and security concerns such as predictable patterns that agents could exploit. We define a ValidationReport schema to formally capture properties like solvability (e.g. reachability via BFS), solution length bounds, branching-factor or degeneracy of decision space, fairness/symmetry between starting positions, triviality (e.g. existence of a one-move win), and anti-precomputation metrics (novelty and complexity). A sample JSON report is provided, along with efficient algorithms (graph search, combinatorial analysis) to compute each field. We also introduce a WatchabilityScoreReport schema to estimate a scenario’s spectator appeal before a match. Building on sports analytics, metrics include “swings” (moments where advantage swings sharply, analogous to win probability swings), “brink moments” (close calls where a player nearly loses, akin to match points saved), “punish windows” (opportunities to punish mistakes, reflecting how long one side is vulnerable after an error), and “interaction density” (frequency of direct engagements). We outline how to predict these using scenario simulations or structural analysis (e.g. lead change potential via game-theoretic models).

In the Threat Model, we identify key cheating or exploitation avenues in an AI-versus-AI setting and how to mitigate them by design. Distribution learning is a risk where agents overfit to the scenario distribution; we counter this with a vast, high-entropy scenario space (ensuring high Kolmogorov complexity of outputs) and even adversarial scenario generation to thwart memorization. Policy precomputation (agents pre-solving scenarios offline) is deterred by making the space of scenarios astronomically large and unpredictable for any given seed. Side-channel leakage (unintended information channels, e.g. timing differences or partial reveals) is addressed via strict isolation and standardized runtime: generation happens before match start, with fixed-time execution so agents can’t infer complexity from delays, and no data about the scenario leaks until the match begins (aside from what is intentionally revealed). Generator inversion (agents deducing hidden parts of a scenario by knowing the generation algorithm) is mitigated by injecting enough randomness and complexity that partial observations don’t narrow outcomes easily – for instance, using encryption or one-way functions in level generation, or multiple stochastic steps, so that even if an agent knows the algorithm, it cannot feasibly reconstruct unseen portions. We also recommend adversarial environment design techniques (as used in unsupervised environment generation research) to continuously evolve scenarios that exploit agent weaknesses while remaining fair.

To enforce seed integrity, we detail a secure commit–reveal protocol. The tournament organizer (or a decentralized process) first computes a cryptographic commitment to the random seed (e.g. an HMAC of a secret key and match ID) and publishes that hash before the match. After both competitors have locked in, the secret is revealed and the seed is derived and verified against the commitment. This ensures the seed was chosen before the match (preventing biased picking) and cannot be altered afterward. For example, the seed could be HMAC(key, tournament_id || match_index), where key is revealed only post-match so anyone can recompute and verify the seed. The commit and reveal phases can also incorporate contributions from both players (each provides a random string, all are hashed together) for multi-party entropy, so no single party controls the outcome. We discuss cryptographic caveats, such as ensuring large enough seed entropy (to prevent brute-force guessing of the commitment) and handling the case where a player withholds their reveal (which would result in an immediate forfeit or use of a public entropy source as fallback). Hardened options include mixing in a public randomness beacon or threshold signatures from multiple referees. The net result is a verifiably fair and auditable seed protocol that preserves determinism (the same seed will always reproduce the scenario) while being tamper-proof and unpredictable in advance.

For Spectator View Projection and Coaching Policy, we formalize a spectator visibility invariant: no spectator or coach receives information about the game state that exceeds what the competitors have, until a fixed delay has passed. In practice, a parameter spectatorDelayMs (e.g. 5,000–300,000 ms depending on game pace) is applied to all full-information broadcasts. This means live public streams are delayed (as is standard in eSports, often by 5–10 minutes to prevent “ghosting”), and any authorized spectator feed is either equivalently delayed or limited to only show information that both players already have (for example, showing only areas of the map that have been explored by both players). We differentiate coaches from general spectators: Coaches are treated as extensions of the team, so a coach’s view is restricted to their own competitor’s perspective (no omniscient view) and without delay relative to their player. We propose two coaching modes: advisory (coach can suggest or highlight actions, but the agent/human competitor ultimately decides and acts) and approval (the agent must get coach’s approval for certain decisions, or the coach can veto/confirm actions). In either mode, the system enforces delivery timing guarantees – for example, a coach’s advisory command might only be delivered during the competitor’s turn or at a fixed frame interval to prevent spamming, and it is timestamped so it cannot “reach back in time” to alter past decisions. All coach communications occur via the official platform channels and are logged for auditability (every suggestion, decision, and the state at that time are recorded). These logs ensure coaching stays within allowed bounds and can be reviewed to detect any illicit information passing. Coaches never receive the delayed full view – their interface mirrors exactly what their player knows (e.g. same fog-of-war, same sensors), guaranteeing that even a coach’s strategic advice cannot come from hidden information. We also impose messaging constraints: the coaching interface could be limited to predefined signals or a rate limit on messages, to avoid sending complex coded information even about the limited view. For instance, a coach might only be able to issue high-level commands (“attack now”, “fall back”) or togglable suggestions, rather than arbitrary free-text, to reduce any covert signaling capacity. These measures collectively ensure that spectators (and especially coaches) enhance the experience without compromising competitive integrity – spectators enjoy a rich view but only after a delay, and coaches can assist their team without ever accessing extra information or real-time enemy data.

Human–Agent Cooperative Play (Exploratory) brings unique challenges. We examine control arbitration models for a human teamed with an AI agent:

Interrupt/Override: the human can take over full control from the AI at will (or vice versa, the AI pauses human input in critical moments). This model guarantees human agency but could be disruptive if done without limits. We suggest an interrupt protocol where the human can seize control only during certain phases or a limited number of times, preventing constant tug-of-war.

Veto Power: the AI proposes actions and the human has a short window to veto them. This ensures no AI action the human deems bad is executed. The risk is time constraints – we define that if the human does not veto within t milliseconds, the AI action is automatically executed (to keep the game flowing). Logged data would record when vetoes happen and ensure the human had no extra info beyond the AI when making the decision (maintaining fairness).

Tokenized Control: each side (human or AI) has a limited budget of “control tokens” or turns. For example, a human might be allowed to play directly for N important decisions in a match, and the AI handles the rest. This enforces a balance and prevents one side (especially the human) from simply overriding the AI continuously. It also provides an interesting strategic layer – the human must choose when their intervention is most valuable.

Action Partitioning: define a priori which types of actions the human handles vs the AI. For instance, in a cooperative shooter, the human might handle high-level strategy or specific tasks (like puzzle-solving or using special abilities) while the AI handles micromanagement or aiming. This division can leverage each party’s strengths. However, it must be well-defined and fixed to avoid unintended information leak. We ensure the AI only bases its decisions on information that a human teammate would also have if they were doing that part, to avoid a situation where the AI’s part of the task uses hidden info and indirectly signals it to the human through its behavior (a potential timing channel).

Competitive implications: In sanctioned tournament play, any human–AI team must be matched against a similar human–AI team to keep fairness (pure AI vs AI, or human+AI vs human+AI). Exhibition matches could explore mixed setups, but for official rankings we recommend homogeneity. Replay integrity is another concern: when a human is in the loop, the match replay must record not just RNG seed and AI decisions, but also the human inputs with exact timing. We require deterministic replays that, when fed the recorded human actions at the same timestamps, reproduce the same outcome. The system should capture the human’s mouse clicks, keypresses, or high-level commands in the timeline. This is logged in a secure way (e.g. cryptographically signing human input events) to ensure they weren’t tampered with after the fact. Anti-cheat measures are critical. One risk is a team claiming to have a human player but secretly using an AI (or a much more powerful AI) – essentially AI masquerading as a human to bypass rules or to gain faster reaction times. To counter this, we propose Turing-test-like monitoring: for example, the human’s interface could include CAPTCHAs or random reaction tests that an AI would fail, and/or the player might be required to be on webcam or biometric input in high-stakes matches. Another subtle risk is the human and AI colluding via timing or patterns to leak info (e.g. the AI slightly delays certain actions as a signal to the human about something it “knows” but the human doesn’t). We mitigate this by disallowing any custom signaling protocols: the AI agent’s behavior is constrained by competition-approved code (reviewed to ensure no covert communication module), and the human’s client software similarly doesn’t expose hidden state. Any suspiciously consistent timing patterns can be flagged by analyzing the logs statistically. We also consider veto abuse: a human could theoretically use a veto system to Morse-code information (e.g. repeatedly vetoing at specific intervals). To prevent this, we can limit veto frequency and require that a veto simply stops the AI’s action without giving detailed alternate instructions (so it’s hard to encode complex messages). Overall, we recommend that official competitive modes disallow unrestricted human influence, instead using one of the structured models above with strict oversight. Freer human–AI interaction can be allowed in exhibition or cooperative modes where competitive balance is less critical, but even there we enforce basic integrity (no leaking of opponent info, etc.).

Per-Stage PCG Algorithm Catalog

We break down the content generation pipeline into four stages – Structure, Fill, Validate, and Score/Select – and recommend 2–3 algorithm families for each. For each approach we discuss determinism (can it be made fully reproducible by a seed?), computational cost, how well it composes in a multi-stage pipeline, failure modes (e.g. unsolvable or degenerate outputs), and security considerations in an adversarial setting. We also note which scenario representations each algorithm suits best (grid-based levels, graph-based scenarios, continuous maps, etc.).

Structure Generation

Goal: Create the high-level layout or topology of the scenario – e.g. the map of rooms and corridors, the graph of connected objectives, or the initial state skeleton. This stage defines the broad geometry or graph that subsequent stages will flesh out.

Constructive Procedural Algorithms (Deterministic Tiling, BSP, Cellular Automata, Maze Carvers): These algorithms build structure through fixed rules and randomness, without global search. For example, a Binary Space Partitioning (BSP) tree recursively splits the area and places rooms, ensuring a non-overlapping dungeon layout. Depth-first maze carving is another constructive method for grid scenarios that guarantees a solvable perfect maze. Cellular Automata (CA) can organically “grow” cave-like structures by iteratively applying rules to grid cells. Determinism: Constructive methods are easily made deterministic by seeding their PRNG; given the same seed they follow the same sequence of splits or cellular decisions. Cost: They are typically fast (linear or near-linear in number of cells/rooms). Composition: They produce a complete layout that can be directly handed to the fill stage. However, they often have parameters (like room size ranges, cellular rules) that may require tuning to ensure downstream fill feasibility. Failure modes: Pure constructive algorithms can produce unplayable maps if not carefully constrained – e.g. a CA might fill the map entirely or disconnect areas if rules create closed-off sections. They might also yield trivial or monotonous layouts (e.g. every maze from DFS has exactly one solution path). There’s no built-in feedback loop to optimize for difficulty or fairness, so the initial output might need heavy validation. Security implications: Constructive methods have a low attack surface since they don’t adapt to players; however, if the rules or seed range are known, agents might predict features. For instance, if BSP always produces a central room or CA caves always have roughly 40% wall density, agents could exploit those patterns. Ensuring enough randomness (and perhaps adding some randomness in choosing which algorithm variant to use) can make outputs less predictable.

Grammar-Based Generators (Graph Grammars, L-systems, Generative Design Grammar): These use predefined rewrite rules to recursively construct levels. For example, a graph grammar can place high-level features (like base, obstacles, objectives) in a graph pattern by applying production rules. L-systems (Lindenmayer systems) can generate branching structures – useful in graphs or tree-like maps (e.g. generating a decision tree of puzzle rooms). Determinism: Grammars are deterministic given the same initial seed and rule-set; randomness comes from choosing which rule to apply when multiple options exist. With a fixed random seed for tie-breaks, results are reproducible. Cost: Very efficient – applying grammar rules is typically linear in the size of the output structure, though uncontrolled recursion can blow up. Composition: Grammar outputs can be high-level (which then need filling in detail) or low-level (directly a full map). They compose well if the fill stage knows how to realize each grammar symbol (e.g. a “room” node from the grammar can be filled with a specific layout by the fill algorithm). Failure modes: A grammar can fail to terminate (or overshoot size) if recursive rules don’t naturally stop – this must be guarded by depth limits. Conversely, it might produce imbalanced layouts if the rules weren’t designed for competitive symmetry (e.g. one branch of the graph ends up much longer than another). Grammar rules being domain-specific is a limitation – they encode assumptions which, if exploited by an agent, could reduce variety (e.g. a graph grammar might always place a treasure two steps from the start due to a rule). Security: Since grammars are rule-based, a competitor could potentially infer the layout if they guess which rules triggered. To mitigate this, one can incorporate randomness in rule selection or have multiple grammars and pick one per seed. Grammars also risk pattern overfitting: if the same few patterns appear frequently, agents might learn to recognize them. Ensuring a rich set of rules (or using parameterized stochastic grammars) increases variation. On the positive side, grammars naturally enforce constraints (like connectivity or resource balance) by design, aiding fairness – e.g. a symmetric grammar can generate perfectly mirrored maps for two players.

Search-Based Structure Generation (Evolutionary Algorithms, Constraint Solvers, Planning): These treat level generation as an optimization problem: generate candidates → evaluate against objectives → iterate. An evolutionary algorithm (EA) can encode a map as a genome (e.g. an array of tile types or graph adjacency list) and use mutation/crossover to evolve layouts that maximize some fitness: e.g. symmetry, connectivity, and interestingness. Similarly, constraint solvers or SAT planners can be given a set of rules (e.g. “there must be a path between start and goal passing through 3 keys”) and will search for a solution layout. Determinism: Though stochastic, these can be made deterministic by seeding the random number generator used in selection and mutation. However, parallelism or floating-point nondeterminism must be carefully handled – e.g. using a single thread or fixed PRNG order to ensure repeatability. Cost: Search-based methods are computationally heavier. EAs might require evaluating dozens or hundreds of candidates over many generations. This may be feasible for small maps or if run offline, but it’s often too slow for real-time scenario generation without simplifications. We recommend using search-based methods either as an offline design tool or in the final select stage rather than raw structure stage (more on this in Score/Select). Composition: They integrate well if the fitness function aligns with downstream needs – e.g. an EA can optimize a graph’s symmetry and distance metrics so that the fill stage has an easier job. One can also hybridize: use a quick constructive method to make an initial layout, then an EA to tweak or partition it for fairness. Failure modes: Catastrophic failure is possible – the search might not find any layout meeting strict constraints (leading to timeout), or it might converge to a local optimum that is technically playable but dull. For example, an evolutionary algorithm might produce a trivial map (all rooms disconnected except one path) if that accidentally maximizes a poorly chosen fitness. We include safeguards like a fallback constructive generator if the search doesn’t find a valid result in X seconds. Security: If adversaries understand the fitness function, they might predict qualities of generated maps. For instance, if they know the generator always optimizes for symmetrical resource placement, they won’t bother searching one side of the map more than the other. The solution is to incorporate some unpredictability in objectives or use multi-objective approaches that produce a diverse set of maps. Also, search-based generation must be hardened against input-based exploits – e.g. if competitors somehow influence generator input (they generally shouldn’t, except via the seed which is secured), a maliciously chosen seed shouldn’t systematically fool the fitness (this is unlikely if seed is random). One security upside: search-based methods evaluate candidates before output, which can include checks for exploitable patterns. This is akin to having an automated test that rejects maps where, say, one spawn point can see the entire map (too advantageous).

Other notable mentions: Noise-based terrain generation (Perlin/Simplex noise) fits structure stage for continuous or grid heightmap scenarios – it’s fast and deterministic, great for natural terrain, but offers little direct control over game-play metrics (so best combined with validators to carve paths or place symmetric start zones). Another is modular room templates: pick from a library of pre-designed rooms and connect them randomly. This ensures human-crafted quality in each piece, at the cost of possible repetitiveness and easy pattern learning by agents.

Mapping to Scenario Types: Grid-based levels (e.g. board games, maze, RTS maps) work well with constructive algorithms (maze carve, CA, WFC) and evolutionary refinement. Graph-based scenarios (e.g. a network of puzzles or an economic game tree) align with graph grammars and search-based graph optimization (like the GA partitioning used to balance strategy game maps). Continuous spatial environments (like a physics arena) often use noise functions for terrain and constructive placement for obstacles, since continuity makes pure graph methods less applicable. In summary, for fast deterministic generation with guaranteed playability, constructive methods and grammars are ideal (especially when combined with symmetry constraints for fairness). For more complex balance criteria, hybrid approaches use a quick structure generator followed by search-based tuning – e.g. generate a base layout then run a genetic algorithm to adjust resource locations for fairness. We will leverage these structure algorithms in stage 4 (Score/Select) as needed to refine scenarios.

Content Fill (Detail Generation)

Goal: Take the high-level structure and populate it with game entities or fine details. If the structure stage laid out the “skeleton” (rooms, zones, graph nodes), the fill stage adds the “flesh”: obstacles, items, enemies, terrain features, etc. This stage often works within local constraints defined by the structure.

Constraint-Based Placement (CSP solvers, backtracking, Wave Function Collapse): These methods treat filling as a constraint satisfaction problem: e.g. place N obstacles and M power-ups such that no two power-ups are adjacent to each other, each room has at most one key, etc. Wave Function Collapse (WFC) is a notable algorithm that fills a grid by propagating local adjacency constraints (much like solving a Sudoku). WFC can generate very intricate, hand-crafted-looking arrangements and ensures all local composition rules are satisfied. Determinism: WFC is deterministic when random tie-breaks are seeded; however, one failure mode is contradiction – sometimes WFC reaches an unsolvable partial state. In that case, one typically retries with a new seed or backtracks. We implement a consistent backtracking strategy with the same seed to either always produce the same result or, if a contradiction occurs, escalate to a secondary fill algorithm (ensuring the pipeline doesn’t stall unpredictably). Standard backtracking CSP solvers (for non-grid placements) similarly are deterministic given a fixed search order. Cost: These approaches can be moderately expensive. WFC operates in polynomial time but with a large constant factor; CSP backtracking can be exponential in worst-case, though in practice manageable if the problem (like placing a dozen items with simple rules) is small. We ensure fill generation is within a fixed time budget (e.g. <100 ms) by limiting search depth or using heuristics. Composition: Constraint-based fillers excel at preserving structure intentions. For instance, if the structure stage says one room is the “treasure room,” constraints can ensure exactly one treasure spawn there and no other high-tier items in neighboring rooms, etc. WFC in particular can fill out terrain patterns while obeying edges of structure (it could take into account pre-placed walls from stage 1). It’s important that the structure passes constraint handles (like marked zones or allowed item slots) to the fill stage. Failure modes: As noted, infeasibility is a risk – e.g. if constraints are too strict (say, a rule “no two obstacles within 5 tiles” and the map is small), the solver may find no solution. Our validator stage will catch an empty or partial fill as invalid, but to avoid this, we design the constraint sets to be satisfiable under normal conditions (and include fallback logic: if a constraint solver fails, relax some constraints or switch to a greedy filler). Another failure mode is lack of diversity: constraint solvers might always pick a similar pattern that satisfies the rules, leading to repetitive outcomes. Randomizing the order of variable assignment or using multiple constraint sets can help. Security: With constraint-based fill, a concern is that patterns in filler outputs could be learned by agents. For example, if WFC is using a particular tile set that always produces a certain motif (like repetitive wall patterns), an agent might recognize that motif and predict the map layout beyond its vision. To mitigate, we ensure a sufficiently large pattern set for WFC (so multiple distinct looks are possible for the same constraints) and possibly inject some randomness (WFC can be biased to explore different solution branches on different seeds). Another security angle: if the constraint solver has a performance quirk (taking longer on certain configurations), an agent might try to infer something from start-up time – but since generation is offline and not observed by agents, this is minimal. Overall, constraint-based fill provides reliable, rule-respecting detail placement but must be managed to avoid unsolvable configurations and overly predictable outputs.

Search-Based or Heuristic Placement (Greedy algorithms, Evolutionary placement, MCTS for items): These approaches treat content placement as an optimization, similar to structure search but focusing on micro-level. For instance, a greedy heuristic could place enemies one by one, each time putting the next enemy in the location that best balances the difficulty (or using randomness within acceptable bounds). Or an evolutionary algorithm for content might encode a particular arrangement of monsters and try to maximize a “fun score” (maybe measured by expected damage or diversity of encounters). Another novel approach is using Monte Carlo Tree Search (MCTS) or other planning to place content: e.g. simulate an agent’s path and place traps where the agent is likely to pass but not immediately see them. Determinism: Greedy algorithms can be deterministic if we break ties with a PRNG. Evolutionary methods for fill (e.g. tuning numerical parameters or shuffling item locations) can likewise use a seeded RNG. They’re usually faster than whole-map EAs because the space is smaller (filling within a fixed structure). Cost: Generally moderate. Greedy methods are linear in items placed. Evolutionary ones might evaluate dozens of fill candidates, but each evaluation could be a quick heuristic (or even a short simulation of combat to gauge difficulty). It’s feasible to do a few hundred evaluations within a second if needed. Composition: Search-based fill can co-optimize multiple objectives that validators care about – e.g. maximize the minimum distance between spawn points (fairness) while maximizing cover near them (to make early game interesting). This can feed into the ValidationReport metrics directly as objectives. They work well after a structure is in place: treat the structure as fixed and just search in the “free slots” for content. Failure modes: A greedy or heuristic placer might produce locally optimal but globally poor layouts. For example, greedy placement might cluster all health pickups in one area because each individually seemed okay, but collectively that area becomes imbalanced. Evolutionary methods might get stuck in a weird configuration that technically optimizes the fitness but is unintuitive (e.g. all obstacles lined up perfectly because the fitness didn’t penalize aesthetics). To avoid this, we incorporate a variety of metrics in the scoring and keep some randomness (to escape local optima). Another failure mode is time overflow: if an evolutionary fill doesn’t converge quickly, we must cut it off and possibly accept a suboptimal fill or revert to a simpler method. Security: If the fill is adaptive (optimized to certain criteria), agents could potentially exploit knowledge of those criteria. For example, if they know the generator always places the strongest enemy in the largest room, they can plan accordingly. Using multi-objective or random objective weights can prevent agents from knowing the exact placement logic. Additionally, an adversary could attempt to reverse-engineer through repeated play (distribution learning): if fill algorithm uses a heuristic like “place key in furthest room from player start”, an agent might infer that after enough samples. We could counter this by sometimes deviating (e.g. 10% of the time, place the key in a second-furthest room) – adding a bit of entropy to placements without making it unfair. From a cheat angle, if fill involves heavy computation like simulation to assess difficulty, we must ensure the agent cannot run the same simulation faster to gain an edge. Since generation is done before the match (and ideally not revealed to the agent until match start), this isn’t directly exploitable at runtime.

Template-Based and Procedural Asset Insertion: Another straightforward family is using templates or random sampling for fill. For instance, for a strategy map, one could divide the map into resource regions and randomly populate each with a predefined pattern (like one of several possible mine arrangements). Or in a card game scenario generator, after structuring the deck, fill in card values by sampling from distributions. This is computationally trivial and deterministic with a seed. Determinism: Guaranteed if using PRNG. Cost: Minimal. Composition: Template libraries can ensure variety and human-designed balance for small sub-parts. Failure modes: If templates are not well-chosen, output can be repetitive or not well-balanced globally. And if random sampling is too naive, it might violate constraints (so typically combined with a validator check). Security: Templates can introduce predictable meta-patterns – e.g. if there are only 5 possible enemy configurations in a room and an agent has seen them all, it can identify which one a new match uses immediately and adjust strategy. To reduce this, have a larger library of templates or parametric templates that scale values. We also guard against player influencing fill – but since players don’t have input, not an issue here except through seed (which is secured).

In practice, we often combine approaches: e.g. use WFC to generate terrain textures or small-scale layout within each room, use a greedy algorithm to place key items based on that terrain, and a constraint check to ensure no two important items overlap. The fill stage ensures the scenario’s playable content meets design intent (e.g. enough ammo, no unreachable power-ups) and sets up the challenge level. By using deterministic but varied methods, we preserve reproducibility while keeping the outputs hard to predict or precompute for agents. Each fill algorithm’s output will be passed to the validation stage for verification of properties.

Validation (Property Checking & Enforcement)

Goal: Analyze the generated scenario to ensure it meets required properties (solvable, fair, non-trivial, etc.), and produce a ValidationReport with detailed metrics. The validation stage is essentially a suite of automated tests and analyses on the scenario. If the scenario fails any critical test (unsolvable, unfair, etc.), the pipeline may reject it and possibly regenerate or adjust content. This stage does not modify the scenario (except possibly minor deterministic tweaks to fix issues); primarily it measures and verifies.

We propose a concrete ValidationReport JSON schema capturing the following properties (each explained with how to compute efficiently):

Solvability and Reachability: Is the scenario winnable or completable by some valid sequence of actions for each side? We ensure solvability by embedding a simple solver or rule-based agent to verify that from the initial state, the win condition can be achieved. For example, in a maze or puzzle, run a BFS/DFS from the start to find a goal. In a two-player competitive game, “solvable” might translate to “neither player is in a guaranteed win or loss state from start” – i.e. both have a path to victory given some strategy. We record a boolean solvable: true/false. If false, the scenario is rejected outright. Efficient computation comes from using domain-specific heuristics; e.g. for a graph of objectives, topologically sort prerequisites to ensure nothing is missing, or simulate a few random games to make sure each player has a non-zero win chance. We also include solution depth metrics: min_solution_length and max_solution_length which denote the shortest and longest path to a win (or to cover all objectives) under optimal play. This can be bounded via heuristic search – e.g. BFS until a goal is found gives min_solution_length, and a modified search (or counting paths up to a cutoff) can estimate max_solution_length. These give a sense of scenario length and complexity.

Solution Branching and Degeneracy: How many distinct meaningful paths or choices exist? A scenario with only one viable strategy is degenerate; one with many branches offers depth. We capture this in metrics like branching_factor_avg (average number of choices per state) and branching_factor_max. For example, in a game tree context, these could be computed by examining the game graph up to a certain depth. In a maze, branching factor could literally be the average number of exits from each junction. We might also compute the count of distinct winning paths (or at least an approximation). These help identify if the scenario is too linear (branching_factor ~ 1) or overwhelmingly complex. Efficiently, one can run a limited-depth DFS and count branches, or use Monte Carlo simulation to estimate how often choices arise. Degeneracy in this context refers to scenarios where despite apparent choices, they don’t matter (e.g. multiple paths that all converge immediately). The validator can detect that by checking if different paths eventually lead to truly different game states or if they merge. We include, say, distinct_paths_count (within some bound) or a boolean has_branching: true/false if below a threshold.

Fairness and Symmetry: Do all players (or all sides) have an equal opportunity given the scenario layout? We specifically measure asymmetries: for two-player zero-sum games, a common metric is the win rate advantage if both players play optimally. We can approximate this by simulating or using simple evaluation: e.g. run a minimax search for a few moves or use symmetrical swap analysis – if we swap the players’ start positions, does the outcome likelihood swap? If yes, it’s symmetric; if one side consistently wins from one position, that’s unfair. We include a fairness_index from 0 to 1 (where 1 means perfectly fair/symmetric). For fully symmetric games (like mirrored starting positions), this index should be 1 by design. If not perfectly symmetric, we might estimate fairness via something like statistical parity of win chances. In addition, we list specific symmetry properties: symmetric_start_positions: true/false, resource_balance: ΔX (difference in resource distribution between sides), etc. For example, “both players have 3 resource nodes within 5 tiles of start” would be symmetry; we quantify any differences. Efficient computation comes from graph analysis: e.g. partition the map into two halves and compare key metrics, or use flow algorithms to see if there’s an isomorphism between the two players’ positions. If the game is not strictly symmetric, fairness can be evaluated by simulating a number of games with a baseline AI playing both sides and computing win percentages. The ValidationReport will highlight any significant bias. If fairness falls below an acceptable threshold (say one side wins >70% in simulation), the scenario is flagged or discarded. Fairness also covers turn order advantage – if going first is a big edge, the report might note first_player_advantage: +5% win rate from analysis.

Triviality: Is the scenario too easy, too solved, or lacking interesting decisions? A trivial scenario might be one where a win can be achieved in one move or without any interaction (e.g. both players start next to a win condition). We include a boolean or score for triviality. This could be inferred from min_solution_length (if it’s extremely low, maybe trivial), or lack of branching combined with short length. Another indicator: if the validator can solve it extremely easily or if a very low-depth search reveals a winning strategy, it’s likely trivial. We might set trivial: true if, for example, the first move has a guaranteed win or if no opponent interaction is needed. Another nuance: scenarios that are effectively decided from the start (e.g. one player has a dominant advantage from placement) are trivial in a competitive sense. Our fairness analysis combined with trivial checks would catch that (unfair and trivial often coincide). The report could contain triviality_score 0–1 or simply flags like no_interaction_required: true meaning one side can win without engaging the other. Efficiently, triviality can be detected by simple rules (e.g. check if starting units’ firepower can immediately reach opponent’s HQ; if yes, trivial).

Dead-ends or Degeneracy: Different from branching, this checks for portions of the scenario that are irrelevant or degenerate. For instance, a large area of the map that a player would never need to visit (a dead-end with no resources) is a degenerate design element. We can compute how much of the state space is meaningful. A measure could be state_coverage – run simulations with random or heuristic agents and see what fraction of the map graph gets visited. If big chunks are always unvisited, that indicates extraneous content (could be fine for exploration games but not ideal for competitive fairness). We might list any dominant strategies detected: e.g. if one branch of choices always yields a better outcome than others, we mark that. This overlaps with branching but is more about if choices are illusory.

Anti-Precompute & Novelty Metrics: How resistant is this scenario to agent precomputation or memorization? We want scenarios to be surprising and diverse so that even an AI that has played many games cannot have a lookup table solution. One metric is scenario entropy: essentially the information content of the scenario relative to the space of all scenarios. We can approximate this by compressing an encoding of the scenario (e.g. run a compress algorithm on a text/map representation; high compression means low entropy). We include perhaps scenario_compression_size or a normalized entropy score. Another metric: edit distance from known seeds – we maintain a database of past scenarios (especially those used in previous tournaments or in agent training, if known) and compute a similarity. If a scenario is too similar to a past one, it’s more likely precomputed. A simple measure is percent of map tiles or graph structure matching any in the past set; our report can include similarity_to_previous: 0.1 (10% overlap with closest past scenario). We also consider the state-space size of the scenario: how large and complex is the game tree emanating from it. This is hard to get exactly, but we can estimate via branching factor and depth as above (a scenario with huge branching and long depth implies a massive state space, which is good for anti-precompute). We might compile an estimated_state_space_size (e.g. branching_factor^depth). Another anti-precompute metric could be difficulty of planning: if we run two algorithms – one that assumes perfect information or precomputation, and one that doesn’t – and see a large difference, it means an agent without precompute is at disadvantage. For simplicity, we include a precompute_resistance score (perhaps derived from entropy and novelty measures). Efficiently, many of these can be calculated statically. For example, computing graph Shannon entropy: treat possible distinct scenario outcomes as uniformly random (not exactly, but we can say the generator’s entropy is high if many bits of the seed actually influence the layout significantly). In practice, simply counting distinct elements – e.g. how many unique tile patterns appear – can serve as a proxy for novelty. The Kolmogorov complexity connection means if a scenario is highly compressible, it’s likely something an AI could internalize. So we aim for scenarios that yield low compressibility (randomized enough structure). Our report might show compressibility: 0.85 (where 1.0 means highly compressible/oatmeal, 0 means highly random).

Sample ValidationReport JSON:

{
"solvable": true,
"solution_length": { "min": 5, "max": 9 },
"branching_factor": { "avg": 2.7, "max": 4 },
"distinct_paths_count": 3,
"fairness_index": 0.98,
"symmetric_start_positions": true,
"first_player_advantage": 0.01,
"trivial": false,
"dead_end_fraction": 0.1,
"precompute_resistance": 0.92,
"similarity_to_previous": 0.05,
"notes": ["All key objectives are reachable by both players within equal distance:contentReference[oaicite:39]{index=39}",
"Scenario is non-trivial: requires at least 5 moves to win and involves confrontation"]
}

This JSON shows a scenario that is solvable, mostly symmetric (fairness 0.98, likely mirrored starts), not trivial (min 5 moves to win, some branching), with high precompute resistance (0.92) indicating it’s novel relative to prior ones.

To efficiently compute these properties, we use a combination of static analysis (graph algorithms, logical checks) and dynamic simulation (running a simplified game model). For example, solvability and min/max solution length can often be obtained by running a search algorithm on the game graph or an abstracted version of it. Branching factors come from exploring one-ply moves at various states. Fairness might require running two simulations with players swapped or applying a known metric like “statistical parity = |P(win|player1start) - P(win|player2start)|”. Many of these computations can be intensive if done naively on a complex game, so we budget time for validation. We prioritize critical checks (solvable, basic fairness) to happen first and fail-fast if needed. More nuanced metrics (like exact distinct path count) can be approximated or cached from offline analysis of similar maps.

If any property violates design thresholds (e.g. unsolvable, or fairness <0.9, or trivial true), the validator will mark the scenario as invalid. The pipeline can then either discard the scenario and generate a new one (with a different seed or by adjusting generation parameters), or attempt a fix. A simple fix might be: if solvability fails due to a missing key, have a post-processing step that inserts the missing key in an accessible location deterministically and update the report (this is risky to keep minimal, since we strive for generation to get it right the first time). The output ValidationReport is attached to the scenario so that the selection stage and match runtime can use it – for instance, the match viewer might display some of these metrics to commentators, and the selection stage definitely uses them to choose balanced, interesting scenarios.

Scoring & Scenario Selection

Goal: Given one or multiple candidate scenarios (potentially generated and validated in parallel), assign a score indicating how suitable or “watchable” they are, and select the best one (or N best for use in a tournament round). This stage uses metrics from the ValidationReport, plus additional watchability heuristics, to choose scenarios that are fair, exciting, and varied.

We define a WatchabilityScoreReport schema that quantifies expected entertainment value. Key components include:

Lead Change Potential (“Swings”): We estimate how likely it is that the advantage will swing back and forth during a match. This can be based on the game’s structure – for example, scenarios that encourage alternating control of objectives or have multiple scoring opportunities tend to produce more lead changes. If we have a game simulator, we can simulate with two evenly matched agents and count how many times the lead (e.g. score or win probability) changes side. Lacking that, structural proxies are used: e.g. the presence of multiple objectives or points of contest suggests higher swing potential than a single objective game. Another measure is using the Vecer-Pollard excitement metric from sports: sum of the total variation in win probability over time. Our system can approximate win probability as a function of game state (maybe by a shallow minimax or learned evaluator) and see how it might evolve. The Watchability report might include expected_lead_changes: 2 and max_swing: 40% meaning we predict about 2 major swings and perhaps at one point a trailing player could catch up by a 40% swing in win probability. High values mean a dramatic game. We base this on known results that games with more lead changes and volatility are more exciting.

Brink Moments: This measures how often the game is on a knife’s edge (one move from defeat) and yet continues. For example, “match points saved” in tennis or last-second base defenses in RTS. Our validator computes something akin to comeback opportunities: e.g. is there a chance for a player to survive at 1 HP or with one piece left and still recover? A scenario encouraging brink moments might be one where defense is strong (so a nearly defeated player can stall) or where objectives are reversible (you can steal back a flag). We quantify this as a combination of comeback window (the depth of states where a player’s win probability was below, say, 10% but they still could win later). If using simulation, count how many times a player goes from “almost lost” to victory. If static, use design features: e.g. presence of safe zones or second-chance resources implies more brink situations. We include something like brink_moment_count: 1 (we expect one critical do-or-die moment) or a probability comeback_chance: 15% (if a player falls behind, we estimate 15% chance they can still come back – higher means more spectator drama).

Punish Windows: In competitive games, an exciting aspect is when one player makes a mistake or over-extends, and the opponent has a window of opportunity to punish that mistake decisively. We evaluate the scenario for such dynamics. For instance, a fighting game stage might be designed so that whiffing a move leaves you open for 2 seconds – that’s a punish window. In strategy or MOBA-like scenarios, a “power spike” or a chokepoint can create windows where a small advantage can be leveraged for a big play. We attempt to measure how large and frequent these windows are. Concretely, we might simulate slight misplays (like one player delays an action) and see how much the opponent can gain (score or position) in response. If the scenario has, say, a narrow bridge which if one player crosses alone and dies, the opponent can rush into their base, that’s a big punish window. We score things like punish_window_avg_duration: 3 turns (meaning on average a mistake leaves a 3-turn opportunity for the opponent to capitalize) and punish_window_severity: high (if a single punish can swing the game outcome significantly). High frequency or high impact punish windows contribute to watchability by creating tense moments where every decision matters. However, if they are too high (one mistake ends the game instantly every time), that might reduce overall enjoyment; so we seek a balance (punish windows that allow excitement without always instant resolution). We’ll incorporate domain-specific analysis – e.g. count how many “free hits” a player can get if the opponent is out of position, etc.

Interaction Density: This captures how frequently the competitors directly interact or contest each other’s actions. A scenario where players are constantly in skirmishes or tactical interaction is more engaging to watch than one where they spend long periods farming or solving separate sub-puzzles. We quantify this as expected number of interactions per unit time or per game segment. If we simulate random or default agent play, count the number of moves that affect the opponent (e.g. attacks, contests over resources). Alternatively, measure distances and barriers: if the starting positions are very far with lots of walls, interaction will be low early (not very watchable initially). If the scenario funnels players together frequently (like central objectives), interaction is high. We produce a metric like interaction_rate: 0.8 interactions/turn or time_between_conflicts: 10s (shorter is more action-packed). To compute without full simulation, we use proxies: path distance between players (short distance implies more interaction), number of contested objectives (more implies frequent clashes), etc. For example, an FPS map might have an “interaction density” measured by the proportion of map area that is visible or reachable by both players within X seconds. We also consider interaction variety – if all interactions are the same type (e.g. always long-range sniping), it might be less interesting than mixed types (close, long, environmental hazards, etc.). We could annotate the report with qualitative tags like interaction_style: high-frequency skirmishes.

Using these metrics and the outputs from validation, we then score the scenario. We might use a weighted formula or heuristic rules. For example, we might define an overall watchability score as:
score = 0.4*swings_index + 0.2*interaction_density + 0.2*punish_opportunities + 0.2*novelty (ensuring fairness is already a gating factor). This formula would be informed by domain knowledge or even learned from data (if we had viewers’ feedback on past games).

The WatchabilityScoreReport JSON could look like:

{
"lead_change_expectation": 2.5,
"max_swing_probability": 0.45,
"brink_moments": 1,
"comeback_potential": 0.2,
"punish_windows": { "count": 4, "avg_duration": 2, "impact": "moderate" },
"interaction_density": 0.8,
"interaction_variety": "mixed",
"estimated_overall_score": 8.7
}

This suggests ~2–3 lead changes, up to a 45% swing in win chance possible, one major brink moment likely, multiple punish opportunities of moderate impact, high interaction rate (0.8 actions per turn involving opponent), and overall a high watchability score of 8.7/10.

We compute these before match execution by using our game model and perhaps assuming a certain level of play (e.g. two near-optimal agents). It’s essentially a theoretical excitement evaluation. Notably, some metrics might require simulating partial games with dummy agents. We keep those agents simple but reasonable (they should contest objectives and not just wander). The computations should be kept light – maybe a few dozen simulated rollouts or a quick Monte Carlo on a abstract state model – to stay within budget.

Finally, scenario selection: if we generate multiple candidate scenarios for a match (say N variants with different random seeds or different algorithmic approaches), we use the ValidationReport and WatchabilityScore to pick one. We eliminate any invalid ones (failed validation). Among valid ones, we can either pick the highest watchability score or do a weighted random selection favoring higher scores (to maintain some unpredictability). Multi-criteria could also be handled by Pareto-front selection (e.g. ensure fairness >= threshold, then maximize watchability, with a secondary consideration for diversity from previous picks). In a tournament setting, we might also avoid repeating similar scenarios: so if one scenario is very similar to one used earlier, we might choose the next best one to increase variety (this ties into anti-precompute by not giving agents the same scenario twice). The scoring stage might also personalize selection: e.g. if we know two particular agents will play, we might avoid a scenario that we know one agent has historically dominated (though in an ideal competition, agents should be generalists; tailoring scenarios per matchup could introduce bias, so generally we won’t do that unless for exhibition).

Algorithm families for Score/Select: We mostly use heuristic ranking, but one could consider a learned model (trained on past matches data) that predicts viewer enjoyment or outcome uncertainty and use that as the scoring function. That would be a machine learning algorithm (e.g. a regression over scenario features to a fun score). It must be fixed (deterministic after training) and carefully validated to not introduce bias. Another approach is a multi-objective evolutionary search in the generation stage: generate a population of scenarios and have objectives for fairness, watchability, etc., then evolve to get a set of Pareto-optimal scenarios and pick one. This effectively integrates selection into generation. It’s costly but feasible offline; online we lean on simpler scoring.

Failure modes in this stage: Choosing a “too exciting” scenario might mean it’s actually highly volatile and could produce upset results (which is fine for spectators but competitors might consider it “unfair randomness”). We therefore always constrain within game design limits – e.g. do not select a scenario that, while high on swings, has a 60% first-player win bias. Another issue is metagaming: if agents somehow know the selection criteria, they might train towards it (though that’s hard – they’d have to become less effective in boring scenarios to discourage their selection? Unlikely). The selection is mostly for the organizers’ benefit. One security consideration: the score calculation uses the game model; if an agent could exploit knowledge of how we compute watchability (for instance, if we foolishly allowed scenarios where an optimal agent can force a dull game but a suboptimal one would produce swings, an agent might always force the dull path – but our evaluation assumes optimal play, so this scenario might slip through with a high score thinking swings will happen, when a super-agent could avoid swings entirely). To mitigate that, we bias toward scenarios that inherently force interaction (so even an optimal agent can’t avoid it because the game objectives require conflict).

In summary, the Score/Select stage ensures that among all valid scenarios, the ones used in competition are the most fair, challenging, and entertaining, quantified by our metrics. This provides not just a better spectacle for the audience but also a consistent level of difficulty for competitors. If our pipeline is overwhelmed (say validation leaves only few choices or time is low), a fallback strategy is to have a reserve of pre-vetted scenarios. For example, keep a “seed bank” of known good scenarios (with stored ValidationReports) and use one of those if generation of a new scenario fails or time runs short. This avoids any match delay or bad scenario. Those curated scenarios would still be used with a fresh seed in commit-reveal, or rotated rarely to avoid repetition.

Threat Model & Security Patterns

(Covered in summary above, but detailed here for completeness.) The platform threat model considers how a determined competitor (or an outside adversary) might exploit the PCG system or match environment to gain advantage. We enumerate major threats and our countermeasures:

Distribution Learning (Meta-Exploitation): Over many matches, an AI agent might learn the probability distribution of scenarios and optimize specifically against it, rather than truly adapting. In extreme cases, if the scenario generator is not rich enough, agents can memorize optimal responses to every likely scenario (“overfitting the test set”). To combat this, we maximize the entropy and diversity of scenario generation. The generator has a huge space of possible outputs (millions of variations), far more than any agent can exhaustively train against. We avoid using a small fixed pool of maps – instead, even if we have templates, we parameterize them so that each match is unique. We also incorporate procedural variability in ways that don’t compromise fairness: e.g. cosmetic variations (different layouts with same difficulty) or hidden random events that agents can’t rely on. Another mitigation is adaptive generation: if an agent seems to have mastered one type of scenario, the system could bias towards others (though this runs risk of not being identical distribution for all agents, so we use this carefully and probably only in non-tournament training contexts). Importantly, we ensure no scenario repeats exactly across rounds unless absolutely needed, so agents cannot simply lookup a previous solution. The validation’s anti-precompute metrics help enforce this (we’d catch if a scenario is too similar to a past one). In essence, a well-designed PCG with a large possibility space acts as a moving target, preventing agents from gaining certainty about what they will face.

Policy Precomputation (Look-up or Tablebase Solutions): If scenarios were drawn from a limited set or were small enough, an agent could precompute optimal moves for every state (like opening books in chess, or tablebases for endgames). Even without full tablebase, an agent could invest heavy offline computation to map out contingencies for likely scenarios. Our approach to thwart this includes:

Large State Spaces: We design scenarios that are complex – e.g. many possible game states and long horizons. The validation ensures a minimum solution length and sufficient branching so that the game cannot be solved by brute force look-up. A scenario that is solved in 2 moves or with trivial strategy is rejected as trivial.

Randomized Details: Even if the high-level structure repeats, we randomize initial conditions (like exact resource placement or unit stats) so that an agent’s precomputed plan must adapt. For example, a maze might be the same graph shape, but the keys could be in different corners each time, so a fixed route won’t always work.

Hidden Information: While our current scope is mostly fully observable games, introducing some hidden information (known symmetrically or revealed via commit-reveal mid-game) can force real-time planning. For instance, a random event seed that both players see at the same time during the match (like a sudden weather change affecting the game) cannot be precomputed because it wasn’t known before match. However, this must be balanced with determinism and fairness; if we do such reveal, it’d be via a secondary seed also secured similarly.

Time Limits: Even if an agent has a plan, it has to compute actions in real-time within time limits per move. We ensure scenario complexity is tuned such that a brute-force search would exceed those time limits. Agents thus rely on heuristics, meaning they can’t guarantee a precomputed perfect play.

Monitoring: If an agent’s moves suggest it somehow knew the scenario (e.g. it beelines through a maze without exploration, far better than chance for a first-time seen maze), it might indicate exploitation or leaked info. We can monitor for suspiciously perfect play on novel maps, though a strong agent might just be generally good. This is more for detecting if our distribution got too narrow inadvertently.
By keeping scenarios rich and partly unpredictable, we essentially enforce that agents must use online decision-making rather than a prepared script for each scenario.

Side-Channel Leakage: This refers to any way an agent could glean information not intended, such as from timing, memory usage, or other indirect signals:

Generation Timing: If scenario generation is done on the same machine as agents, an agent might measure how long the initialization pause is and guess something (e.g. maybe certain map types take longer to generate). We mitigate this by standardizing the initialization time – if a map is generated faster than a certain threshold, we intentionally wait until a fixed “match start” time so the agent just sees a consistent delay. Additionally, generation can be done on a separate server or thread the agent has no access to, and only the final scenario is fed to the agent when the match begins.

Memory Footprint or ID patterns: We ensure that any identifiers (like random IDs for objects) are either randomized or not exposed to agents. If agents see something like “Object42” vs “Object7” and glean that it correlates with map type, that’s a leak. We use normalized descriptors (or none at all).

Networking or I/O patterns: If the competition environment transmits the map to agents, we send it in a fixed size/order format every time. For example, if a larger map results in a larger message and an agent could read message length before fully parsing – we avoid that by sending a constant-size buffer (padding if needed). Similarly, any file names or seeds given to agents are sanitized. Ideally, agents get the full state in one go so there’s no piecewise reveal that could be timed.

Spectator/Caster leakage: Coaches and spectators are on delay and restricted view as discussed, so an agent can’t get live help. Also, in a human-AI team, the human’s client might have extra info (like outlines of their own units behind walls). We must ensure the human cannot intentionally or accidentally feed that to the AI agent (the agent’s sensors are separate).

Hardware side-channels: This is quite low-level (like measuring CPU cache or power). In cloud environments, we treat each agent in isolation (different processes, ideally containerized or sandboxed VMs) to prevent them measuring each other or the host’s operations. Also, the commit-reveal seed ensures no one can guess the random seed by e.g. measuring system clock (the seed is derived cryptographically, not just time).
Academic work on side-channels shows many creative attacks, so we adopt a zero-trust approach: the agent should receive only the intended inputs (observations) and nothing else. Logging and auditing helps to ensure this.

Generator Inversion or Reverse-Engineering: If the agent knows the generation algorithm (which we don’t consider secret – it might be public knowledge in rules), could it infer hidden parts of the scenario from the visible parts? For instance, if the agent recognizes it’s in a map generated by Grammar X rule Y, it might deduce “the treasure must be in the far right corner because that’s how the grammar works”. To counter this:

We introduce stochasticity and variability in generation such that many different outputs look similar initially. For example, many mazes might start with the same entry area, but diverge later. The agent can’t be sure which one it is until it explores. By then, it has to actually play rather than rely on prior knowledge.

We can occasionally use multiple generator variants: say 50% of the time use algorithm A, 50% algorithm B, where outputs look similar on the surface but have different hidden structures. The agent cannot as easily invert because it’s unsure which algorithm was used (commit to not revealing that).

If feasible, encrypt hidden information. For instance, if there’s a hidden item location that agents shouldn’t know, we ensure it’s truly not deducible except by directly searching in-game. (In deterministically seeded games, one worry is if the agent somehow extracts the seed from some part of the observation. We ensure the seed is never part of the observable state in any straightforward way.)

The commit-reveal scheme prevents an agent from knowing the seed ahead of time. If an agent had the seed, since the generator is deterministic, it could theoretically run the generator itself and map out everything. But seeds are revealed effectively at match start only. If the agent is an AI program, it doesn’t get an opportunity to rerun the generator unless it was trained to somehow compute it on the fly within its computation budget. We assume that’s infeasible for complex generators (and would be detected).

We utilize property randomization: e.g. even if the map topology is known, specific values like exact enemy HP or puzzle codes can be random each time. An agent might guess structure but not these fine details.
In summary, even though the generator is deterministic and possibly public, we aim to design it such that partial observation doesn’t yield full knowledge. This is akin to “don’t place all secrets in one basket” – diversify where random uncertainty lies in the scenario.

Collusion and Information Sharing: Outside the direct PCG, a threat in tournaments is players colluding or sharing info about seeds or scenarios (if the same scenario might be reused in different matches). We mitigate by not reusing scenarios and by using commit-reveal per match so even staff cannot bias a seed for a certain player. If multi-round events required same scenario for both players (to be fair in evaluating performance), we’d still hide it from them until needed, and isolate any who saw it from those yet to play (like how exams are handled). But our focus is more on procedural fresh content each time.

Cheating via Coach/Operator: We covered coach limitations. Also, we ensure no “coach bot” could eavesdrop on spectator feed beyond allowed (which is prevented by delay and isolating networks). For human players, typical anti-cheat like ensuring they’re not running unauthorized software is needed, but with AI players, the main cheat vector is the code itself (which is provided and sandboxed).

To mitigate threats by design, we incorporate patterns:

Commit-Reveal for fairness and unpredictability: as detailed, using HMAC or similar for seeds. This stops any tampering or early knowledge.

Audit Logs: We log generation seed, the ValidationReport, and match logs. After matches, these can be published (or at least available to judges) to see if anything anomalous happened. If a scenario turned out to be unbalanced, we can analyze why our metrics missed it and improve.

Diversity enforcement: The generator might explicitly track recently used scenario features and avoid repeating them (to reduce distribution learning). For example, if three games in a row all had a high-ground advantage on left side, the generator might avoid that next time. This is a bit dynamic and could be perceived as non-stationary distribution (which is fine as long as it’s consistent for all players in the long run).

Secure Execution: Running agent code in a sandbox so they can’t, say, read system entropy or external files to guess seed or communicate out-of-band. They should only communicate through the official match APIs.

Modern research like DRAGON/PAIRED environment generation shows even adversarial generators can be harnessed to improve agent robustness. We borrow from that philosophy: by making our generator unpredictable and occasionally adversarial (within fair bounds), we ensure no agent can reliably game the system. The flip side threat is if the generator becomes too adversarial, it might generate unsolvable or extreme scenarios (like the minimax adversary that made impossible environments). Our validator prevents that by requiring each scenario is winnable and not unreasonably biased. PAIRED’s solution was to ensure environments are solvable by an antagonist agent – similarly, our validation ensures at least some agent (even a simple one) can solve it, so it’s never inescapably unfair.

In conclusion, our threat model approach is preventative (secure seeding, no leaked info, huge scenario space) and detective (logging and validating to catch anomalies). Combined with a strict spectator/coach policy, these measures uphold competitive integrity even against sophisticated AI opponents.

Seed Integrity Protocol (Secure & Auditable Seeding)

To guarantee fairness and determinism, every match’s procedural generation uses a single master seed that is decided in a provably fair manner. The requirements are: (1) Neither competitor (nor anyone) can influence the seed to their advantage, (2) Neither knows the seed until the game starts (preventing pre-simulation), (3) After the match, everyone can verify what the seed was and that it was used correctly.

Our chosen approach is a commit–reveal protocol using an HMAC (Hash-based Message Authentication Code). Here’s how it works step by step:

Seed Derivation Inputs: We define a string or data block that will be the basis for the seed. For example: match_id || tournament_round || map_index || optional_salt. The match_id is unique per match. We also include a tournament_secret – a secret key known only to the organizer (or a secure service). The seed for the game is then computed as:
seed = HMAC_SHA256(tournament_secret, match_id || round || map_index) (and truncated/converted to an integer of desired bit-length).
This HMAC is essentially a pseudorandom function output that is unpredictable without the secret.

Commit Phase (Pre-Match): Before the match, the organizer publishes the commitment, which in this case can be the hash of the seed. If using HMAC as above, a straightforward commitment is commit = hash(seed) (or we could even use the HMAC output itself as the commit, but typically commit–reveal has separate steps). Alternatively, we can commit to the secret key instead: For instance, publish H = SHA256(tournament_secret || match_id || ... ) which implicitly commits to the seed since the secret will later be revealed. A simpler method many systems use is: generate a random 64-char server seed, publish its SHA256 hash as commit, then later reveal the seed. We can adapt that here:
Organizer generates random server_seed_R. Compute commit = SHA256(server_seed_R). Publish this commit before players submit any bots or moves for the match (e.g. at match scheduling time). At this point, no one knows server_seed_R except the organizer, and the commit ensures it’s locked in (the organizer can’t change it later because that would break the hash match). The players could also contribute a client seed each in commit if desired (more on multi-party below).

Reveal Phase (Match Start): At the designated match start (or after both players are ready and have potentially provided their own commitments if that scheme is used), the organizer reveals the secret(s). In HMAC scheme, reveal tournament_secret (if it’s per match, could just reveal the seed directly). In the random server_seed scheme, reveal the actual server_seed_R string. Now everyone can compute the hash of this revealed value and confirm it matches the earlier commitment, proving this is indeed the original secret chosen before the match. Then the final game seed is derived. If using the 2-party method:
game_seed = SHA256(server_seed_R || client_seed_A || client_seed_B) for example, combining both players’ contributions (after both revealed). This combined seed is then used in the PCG deterministic generation.

Usage of Seed: The PCG pipeline is deterministic given this game_seed. That means all randomness in structure/fill generation is drawn from a PRNG initialized with game_seed. For reproducibility, we record this seed in the match log and the generation process. After reveal, since the match immediately starts, neither agent has time to do extensive computation on it beyond what they’re allowed in real-time. And since commit was done before they locked in, they couldn’t have tailored their strategy to a particular seed.

Cryptographic Caveats: We use strong hash functions (SHA-256 or better) and sufficiently long secrets (256-bit) to prevent brute force. The commit must be published in a way that players trust (e.g. on a public dashboard or blockchain or at least a timestamped server message) so the organizer cannot secretly commit multiple and only reveal the one they like. HMAC-SHA256 is secure, meaning that without knowing tournament_secret, the seed is unpredictable. We do have to ensure tournament_secret is not reused too broadly; often a fresh random key per match is simplest (which reduces to the server_seed scheme). If a fixed secret were used for all matches, revealing it once would allow predicting all seeds – so we wouldn’t do that until maybe the end of tournament. Better to treat each match separately.

Multi-party entropy (optional): To further remove any single point of trust, we can let each player contribute randomness. For instance, before seeing the organizer’s seed, each player picks a random 128-bit string, hashes it and sends the hash as their commitment. Once both commits are posted (simultaneously ideally), they reveal their seeds. Then we XOR or concatenate them with the organizer’s seed to form the final seed. This way, as long as at least one party (organizer or either player) was honest and random, the final seed is random. Even if one player tried to bias it, without knowing the others’ contributions they can’t force a predictable outcome. The commit phase prevents players from choosing seeds after seeing others’. One known attack is if the last party to reveal could try many seeds until they get a favorable outcome – to stop that, either make reveals simultaneous or have an authoritative secret that even last revealer can’t influence final outcome (e.g. final seed = HMAC(secret, A_seed, B_seed) so organizer’s secret always adds unpredictability that players can’t manipulate). Research suggests commit–reveal with two parties is robust if both commit first, but if one fails to reveal, we need a policy (likely forfeit or use the other’s seed alone plus organizer’s).

Public Randomness (alternate/hardened): We could also incorporate an external random beacon (like NIST randomness beacon or a decentralized source). For example, at match time, take the latest Bitcoin block hash or NIST beacon value and combine with our seeds. This ensures even the organizer can’t fully predict or control it. However, relying on external sources can complicate scheduling (we’d need them at the right time). We treat it as an extra layer if needed.

Commit-Reveal Timing: Ideally, commit happens well before match (e.g. when the bracket is announced) to eliminate any suspicion of late changes. Reveal happens seconds before generation (or even after generation but before match starts, since generation is quick). In practice, we might generate the scenario after reveal (since at reveal we get the seed). If we wanted to generate scenarios earlier, we’d have to do commit->reveal earlier too which undercuts unpredictability. So, better to generate essentially at match start with the now-revealed seed. This is fine since generation is within our time budget.

Auditability: After the match, anyone can take the revealed seed and run the PCG code (which we can even open source) and verify the exact scenario was produced. This transparency builds trust. Also, we archive all seeds and commitments for record-keeping. If any dispute arises (e.g. a player thinks the organizer gave a biased map), they can check the seed reveal and confirm it wasn’t tampered (and because it was partly their own seed or public random too, it’s clearly fair).

We also discuss hardened paths: For the ultimate in trustless fairness, one could do everything on-chain or via threshold cryptography. For example, each player and the organizer could be nodes in a verifiable random function (VRF) scheme or run a protocol like drand (distributed randomness) to generate a public random seed. These ensure no single party can determine the outcome and the result is publicly verifiable. This might be overkill for most tournaments, but it’s an option. A simpler hardened path: use multiple independent commit-reveal: say organizer and a neutral third party (like a referee or even an oracle service) each commit seeds and the final seed is combined. Then even if one is malicious, the other’s randomness protects the result.

In short, our protocol is similar to those used in provably fair gambling and blockchain raffles: commit to a random value, later reveal it, and mix to get outcome. It’s simple operationally (just generating and hashing seeds) and very secure. The exact derivation as recommended:

server_seed = random128bit()
commit = SHA256(server_seed)

# publish commit

# players optionally do similar with player_seed_A, player_seed_B

# at match start:

reveal server_seed (and player seeds)
combined = server_seed || player_seed_A || player_seed_B || match_id
game_seed = SHA256(combined)

This game_seed is then used by the PCG as the root for all randomness.

We also define how it’s integrated: The tournament system likely automates this: when a match is created in the system, it generates and stores server_seed (secret) and posts the commit publicly. When match is about to run, it retrieves server_seed, gets any player seeds (if used; if players are AI code, we might skip player seeds for simplicity – human tournaments have used player seeds in some cases to avoid blaming RNG), and computes the final seed. Then it feeds that to the game instance. The reveal (server_seed and any others) can be posted log or even in the match transcript so it’s visible. We caution that server_seed should be rotated every match (as Stake’s system does – once revealed, a new one is generated for next match).

This protocol prevents seed manipulation and foreknowledge: no competitor can know the seed early (so no practice on that exact scenario), and the organizer cannot skew the draw after seeing players or results because they committed to hash beforehand. This approach is well-grounded in cryptographic practice and in use in eSports (for random map bans or coin flips, sometimes blockchain commitments are used).

Spectator View & Coaching Policy

Modern competitive integrity requires that information separation is maintained: competitors should not gain extra knowledge from spectators or coaches beyond what the rules allow. At the same time, spectators (audience, casters) want a rich view of the game. We formalize a spectator visibility invariant: At any point in time, no spectator or coach has access to game state information that a competitor is not allowed to have, unless a sufficient time delay has passed such that the information is no longer strategically useful. Invariant means this holds throughout the match. A practical interpretation: spectator feeds are either limited to each player’s perspective or delayed. We implement two types of spectator views:

Public Broadcast (Omniscient) with Delay: This feed shows the full game state (all players’ positions, hidden info, etc.) but is delayed by spectatorDelayMs which might be on the order of tens of seconds or minutes. The delay must exceed any reasonable decision window. For example, if the game is turn-based, a delay of one full turn or more ensures no spectator can relay info to affect the ongoing turn. In many eSports, 5 minutes is common because that covers even long engagements. This ensures by the time an audience sees an ambush or a strategy, the players have already encountered it themselves.

Live Restricted View (POV Spectating): Authorized viewers (like approved commentators or coach feeds) can see the game live but only from a specific player’s perspective. Essentially, they get the same observations that player gets (perhaps with a nice UI, but no extra data). This means they are always at most as informed as the player. If they tried to tell the player something, it’s presumably something the player could also know.

We set spectatorDelayMs based on game parameters. If our platform supports both modes, public streams definitely use the delay. The invariant is maintained by the system – e.g., our match server might have two spectator ports: one that buffers output for delay, one that filters output.

Coaching Policy: We allow coaches under certain modes but with strict rules to avoid them being a side-channel for cheating. Two modes defined:

Advisory Mode: The coach can send advice to the competitor, but the competitor (AI or human) is not forced to follow it. For a human player, this is like a traditional coach giving suggestions during play (if allowed by rules). For an AI, this could be implemented as: the AI periodically queries the coach for a hint or the coach can recommend an action, but the AI decides. The key is that the coach’s information is limited to the player’s info. The coach UI only shows what that player sees (and possibly some analytics on that, but nothing the player couldn’t derive). The coach’s advice channel is structured – e.g. maybe a textual message or selecting a unit and target to suggest an attack.

Approval Mode (or Veto Mode): The coach must approve certain decisions or can veto them. For example, an AI might compute a move and ask the human coach “Is this okay?” and the coach can approve (let it execute) or disapprove (suggesting the AI picks another move). Alternatively, the human might be playing and the AI agent is the “coach” in the background, flagging risky moves or requiring confirmation on big decisions. This mode is tricky in real-time – it works best in turn-based or where slight pauses for approval are acceptable. We implement it such that there’s a time limit for approval – if coach doesn’t respond in e.g. 2 seconds, the action is auto-approved or a default safe action occurs. This guarantee prevents stalling (a coach could otherwise freeze the game by refusing decisions, which is not allowed).

Delivery Timing Guarantees: Coach messages must arrive at the competitor without lagging behind the game state in a problematic way. For advisory, it’s okay if advice is a second late (the player might ignore if outdated). But for approval, it’s synchronous to the action. We ensure the network or system scheduling prioritizes coach communications. Also, we might limit how frequently a coach can intervene to avoid flooding the player with instructions (especially for AI coaches sending to human players – too much advice could basically turn into the AI playing, which might break rules). We guarantee that if the coach sees something at time T (according to their limited view), their advice based on that will reach the player by T + δ with δ small. We also log the timestamps: if a coach message arrives too early (somehow before the coach could have seen the trigger, suggesting cheating), that’s flagged.

Enforcement of Limited Coach View: The system ensures that the coach’s client or interface subscribes only to the player’s data channel. For example, in a MOBA, a coach might have a special client that is essentially a spectator locked to only show their team’s vision (no enemy fog of war info). We implement this by tagging all game state data with visibility and filtering at source. Additionally, coaches could be required to physically sit with players (in human tournaments) so they see only what the player sees on screen. In our AI context, the coach is likely another AI process or a human on a dashboard – either way, the server simply doesn’t send them forbidden info.

Logging and Messaging Constraints: Every communication from coach to player and vice versa is recorded in detail (content and timing). This log is part of the match record for potential review by officials. To prevent sneaky encoding of forbidden info, we constrain the form of coach messages. For instance, we might restrict to a predefined set of signals: “attack now”, “fall back”, “focus left”, “focus right”, “use power”, etc. A coach thus cannot send an arbitrary code like “X42” that might correspond to “enemy at (4,2)”. The limited vocabulary means they can only convey general strategic concepts that the player already somewhat knows (just maybe not the timing). In an AI coach scenario, the AI is likewise constrained: maybe it chooses from a fixed policy set rather than sending raw data. Also, no outside devices: the coach (human) can’t have another screen with the stream or use a phone to get external data. Tournament rules mirror those in traditional sports: during a live match, coaches and players should be isolated from any external communication except their own allowed channel.

We also specify that coaches cannot interact with the game directly. They cannot click or press keys in the player’s stead (unless the mode is explicitly some shared control which would then count as the human-agent team scenario, not just coaching). They only advise.

For AI coaches specifically, they might be an algorithm running on the player’s machine or as part of the agent. That blurs lines (it could basically become part of the agent). If allowed, we consider that as part of the agent’s design (some agents might have “coach modules”). In competition, that’s fine as long as the module doesn’t use extra info. If an AI coach were separate (like a second agent program), it must abide by same info rules. One enforcement mechanism: run the coach code in the same sandbox with the same observation feed as the main agent; then it’s literally impossible for it to get more info.

We further address integrity by scheduling pauses or timeouts if needed. Some competitions allow a coach to talk to players only during timeouts or breaks. Our system could similarly restrict coaching to specific phases (like only in between rounds or during a planning phase). This is another way to ensure no constant stream of hints. If we choose continuous coaching, the above constraints apply strongly.

Finally, “spectatorDelayMs semantics” means if any spectator feed (like a live stream) is accessible to someone who could communicate with players, that delay must exceed any potential advantage window. E.g., if a coach theoretically had a second device showing the public stream, the delay should be large enough that whatever they see is old news. This is already enforced by isolating coaches to team view, but the general rule is always consider worst-case: even if a player’s friend in audience tried texting them info from stream, the delay prevents it from being useful. Hence the invariant holds.

To illustrate, consider the CS:GO coach cheating scandal: coaches abused a spectator bug to see the whole map in real-time. Our system design would outright prevent that – a coach client could never free-fly the camera. And if somehow a bug occurred, it would violate the invariant and be detectable (coach would suddenly have info they shouldn’t, which if used, shows in logs). ESIC’s heavy sanctions on that underscore how seriously we must treat it. Therefore, we implement a “zero-trust” approach: assume any extra info will be abused, so don’t provide it at all.

In summary, coaches see and communicate only what their competitor already could know, and spectators who see more get it later, so the competition remains fair. All such interactions are logged and governed by rules to ensure they cannot act as unintended or unauthorized oracles of hidden information.

Human–Agent Cooperative Play (Exploratory Considerations)

Looking ahead to modes where humans and AI teammates compete together, we analyze how to integrate their inputs while preserving fairness and preventing abuse. The core challenge is action arbitration: deciding in real-time which of the human or AI controls which aspect of the game, and how to switch or blend control.

Arbitration Models:

Interrupt/Cede Control: Either party can grab control from the other. For instance, an AI could be driving a character and the human hits a button to take over manually. Or a human is playing and the AI detects a complex situation and takes control for a few seconds (if allowed). This model requires clear rules: maybe the human is primary and can always interrupt the AI (like auto-driving that you can override by grabbing the wheel). Or vice versa, AI does routine tasks and yields when human wants. We must log each such control switch to ensure no unauthorized pattern (like an AI taking over in critical fights without permission). We also likely limit frequency of interrupts to avoid confusion (e.g. you can’t flicker control rapidly to confuse opponent or to try multiple actions at once).

Veto/Approval (Mixed Initiative): The AI suggests an action (or auto-performs unless vetoed). This is akin to an AI assistant. The human can at any moment say “no, don’t do that” which either stops that action or forces a recalculation. Or the human plans and the AI vetoes moves that are obviously bad (if such is allowed). This requires a trust setup; too much vetoing slows the game. Usually, one side is given the final say to avoid stalemate (commonly the human has final veto in designs, for responsibility and fairness).

Tokenized Control: Here, control is discretized into tokens or turns that each side can spend to take charge. For example, in a time-slice sense, a human could have 5 tokens that each allow them to control 10 seconds of gameplay (like clutch moments), the AI runs the rest. Or in a turn-based game, maybe the AI plays the opening and endgame while human plays critical midgame by prior agreement or token use. This model ensures both contribute but prevents one from completely overriding. It also adds strategic depth: e.g. the human must judge when to step in.

Action Partitioning: Split responsibilities by type. E.g., in a strategy game, the human handles high-level strategy (building, unit production) and the AI handles micro-management of units in combat. Or in a racing game, the human steers on straights, AI on corners (just hypothetical). Many robotics shared-autonomy systems do this: the human controls some degrees of freedom, AI the others. This can play to strengths (human strategic intuition + AI reflexes). It needs a well-defined interface so they don’t conflict. For instance, if human and AI issue conflicting commands to units, who wins? We might designate certain unit groups to AI and others to human, or human sets goals and AI executes them. The design must be public and fixed, so all competitors know what is allowed.

Competitive Implications:
If human–AI teams face purely AI teams, there’s asymmetry. Humans bring creativity but slower reaction; AIs bring precision but possibly predictability. Without mitigation, one side could have advantage. Likely, competitions would either separate a league for human-AI teams or ensure both teams have equivalently one human and one AI (so it’s symmetric). Assuming two human-AI teams face off, fairness between them is fine. But we must consider exploitation: could an AI abuse the human partner or vice versa in a way that violates rules? For example, using the human as an “oracle” for NP-hard planning (the human effectively computing something the AI couldn’t). In principle, that’s allowed teamwork (not cheating) – it’s actually a goal: synergy.

However, what if the human uses external help? That becomes cheating akin to a human player cheating. So same rules: no external comm beyond allowed.

Replay Integrity: With two agents (human and AI) controlling one side, reproducing a match means we need to replay both their inputs. We must capture not only the random seed and one player’s actions, but all human inputs (key presses, etc.) with timing, plus maybe internal AI decisions if they depend on non-deterministic processes. The solution: the human’s inputs are recorded as a deterministic sequence of events (just like we record all actions in the game). On replay, we feed these at the exact timestamps to the game engine (representing the human) while the AI portion runs deterministically from the same seed. This should recreate the game. If the AI’s decisions might have been influenced by human in a complex way (like if the AI listens to voice commands), we record those intermediate signals too. Essentially, treat the human as another “input source” that we log. As long as the same sequence of human input and initial seeds are applied, the game replay remains deterministic and verifiable.

Anti-Cheat Measures in Human-AI context:

AI masquerading as human: A team might claim a human is playing with an AI assistant, but actually let the AI do 100% of the work (the “human” is just watching or, worse, feeding extra data to AI). This could happen if humans are required but an org tries to field two AIs to gain more compute. To prevent this, if a human is supposed to be doing certain actions, we monitor their input. If logs show the human provided almost no input (and all moves came from AI), that’s suspicious. Tournaments might require humans to perform at least X actions or take control during certain phases. There could also be biometric or camera verification of human activity. In online scenarios, we might have to trust but verify via stats: e.g. if a team’s “human” consistently outperforms what any human could (reaction times of 50ms consistently, etc.), it hints they are letting AI take over. They could face disqualification. This is similar to detecting cheating in online chess (an “advanced” style beyond human capability).

Human feeding external info to AI: E.g. a human could have knowledge the AI doesn’t (maybe from the environment or rules) and feed it through subtle inputs. Since it’s a teammate, that might be allowed if it’s normal teamwork. Actually, a human is allowed to tell their AI partner anything they deduce (just like two humans on a team can talk). That’s not cheating; it’s collaboration. The only concern is if the human got info by illicit means (like spectators). Which we prevented earlier. So within a team, free communication is usually allowed (maybe except using coded languages unknown to referees in some sports, but here likely fine).

Timing Channels between AI and Human: If the AI wanted to leak some hidden info (like it knows the map seed internally but the human doesn’t), it could encode it in its behavior. For instance, it could move in a pattern that signals a letter. This is very hard to police if they truly collude, because any action could be a signal. In a strict view, since they’re on the same team, sharing info is actually not against rules – teams are allowed to share all knowledge among members. The only caveat: the AI shouldn’t have had any info the human couldn’t anyway (due to the spectator rules). If it did (which it shouldn’t), that would be a leak. So assuming no illegal info, any signaling is just teamwork, not cheating.
We might worry if, say, a human on one team tries to signal the AI on the opposing team by exploiting some game mechanic – that’s cross-team collusion, disallowed similarly as between humans. But on the same team, it’s fine.

Design Recommendations (Sanctioned vs Exhibition):
For official competitive play, we likely restrict the roles to ensure fairness and minimize chaos:

Possibly enforce that the human is the primary with veto power, to avoid the AI doing superhuman micro that a human team without such an AI couldn’t match. If both teams have equal human+AI, maybe it’s fine. But if one team’s human is doing everything and another’s is letting AI micro, that could be an imbalance if not controlled by rules. Maybe rules like “the human must execute all final actions, AI can only suggest” could level it. But that also diminishes the AI’s contribution. This is an open area. Perhaps better: require the AI to operate at “human speed” (no unrealistic APM). You can enforce a cap on actions per minute or require random small delays in AI actions to simulate human limitations, so that having an AI doesn’t circumvent mechanical limits.

Might also restrict the types of AI assistance: e.g., allow strategic advice but not fine control (so AI can’t aim for the human, but can do strategy planning).

In exhibition or casual play, these restrictions can be looser to demonstrate cool interactions. For instance, a showcase match might let a pro gamer and an AI both fully play together unrestricted (which could be very strong, but it’s for show).

Also, rules for substitutions: maybe an exhibition allows an AI to step in if a human disconnects, or vice versa, just as a demonstration.

We should also consider ranking and integrity: If humans + AIs compete, should they be rated separately from full AIs? Probably yes, since the dynamics are different (like centaur chess ratings differ from pure engine ratings). For fairness, one might not mix them in the same bracket or if they do, give guidelines (like maybe a slight handicap if one side is human+AI vs AI to account for human lag, though better to avoid mixing).

Cheating unique to this scenario: One example – a human could secretly let an AI system (outside the allowed one) assist them, essentially having two AIs. That’s like a human doping with AI. Or an AI could coordinate with an external human via covert means (maybe not realistic as AI doesn’t have external com, unless coded in). Our logging of all communications and enforce isolation prevents that mostly.

Finally, from a design perspective: we want the cooperative mode to be fun and competitive without breaking things. So we likely will:

Provide official interfaces for human-agent communication (so no need for shady channels).

Possibly have a referee AI or monitor to ensure compliance (like an AI analyzing if a human reaction time is impossible or if an AI is spamming beyond humanly possible moves).

As technology, consider using imitation learning: have the AI learn to calibrate to human’s style (some research on shared autonomy does this, blending controls smoothly). This ensures a more cohesive team instead of fights for control.

In summary, we propose clear role assignments in human-AI teams (e.g. human leads, AI assists, or vice versa, or role-split by tasks), enforce with in-game mechanics and rules, and keep extensive logs to catch any deviations. Human-AI co-play should be introduced in exhibition matches first to refine these policies (because unforeseen issues may arise), and only once fairness and anti-cheat measures are thoroughly tested should it be considered for sanctioned competition. The replay analysis, biometric checks, and action caps form a robust anti-cheat framework to maintain integrity even as we blur the line between human and machine players.

Performance Budgets & Benchmarking

To ensure our platform runs smoothly, we establish performance budgets for scenario generation and validation, and we plan benchmarking procedures to adhere to them.

Per-Scenario Candidate Counts (N): We often generate multiple candidates and select one. How many is practical? We recommend N = 3–5 candidates for most cases, generated in parallel or sequentially within the allotted time. Through testing, we find that a handful usually suffices to get one that passes all validation and has good watchability. Generating too many (like 50) could marginally improve quality selection, but at cost of time and possibly enabling “cherry-picking” bias (we want some randomness in selection to avoid over-optimization of scenarios). If generation is very fast and we want the absolute best, we could go up to maybe N=10 and pick top. But initially, target ~3 candidates and pick the top scoring one.

Timing Expectations: The entire pipeline should ideally complete in under a few seconds, to not delay matches. If matches are started one by one, we can allow up to say 2 seconds for generation+validation, because it’s done once at match start. If doing this in between rounds of a game, possibly need shorter (like some games generate a new map each round in seconds).

Structure generation: Most constructive or search algorithms can run in tens of milliseconds for reasonable map sizes. Even an evolutionary algorithm might be able to complete in under 500 ms if it’s limited generations. We budget, say, 100 ms for structure on average.

Fill generation: WFC or constraint solving might be the heaviest. WFC on a 50x50 map could take maybe 100 ms to a few hundred if backtracking a lot. We budget ~200 ms for fill. If fill fails and retries, maybe a bit more.

Validation: This can involve BFS search or simulations which might be heavy. But we can optimize: for example, BFS on a graph of a few hundred nodes is <50 ms. Simulating a few hundred playouts of a game might be heavy if the game is complex, but we can likely do a shallow simulation quickly (or reduce the state complexity for simulation). We aim for validation within 300 ms. Possibly multi-thread some checks.

Scoring: This is part of validation basically. Computing watchability likely uses the same simulation data. So included in that 300 ms.
In sum, each candidate scenario might take up to ~0.5–0.7 seconds to generate and validate. With N=3, that’s ~2 seconds worst-case. We should also include some overhead for logging, seeding, etc. So target <2 seconds. For reference, some PCG in games can even be done in real-time per frame, but our tasks are more analysis-heavy.

We also plan for worst-case outliers: e.g. if WFC hits a contradiction and retries many times, it could spike. We implement a timeout or fallback: e.g., if fill isn’t done by 500 ms, abort that candidate (mark it failed and move to next). Or switch algorithm: maybe after failure, use a simpler filler to guarantee completion, albeit perhaps lower quality.

Fallback Strategies:

Simplify Content: If generation is struggling (say we tried a fancy constraint fill and it keeps failing or timing out), we can fallback to a simpler random fill that we know always succeeds quickly, then rely on validation to ensure it’s not too bad. Similarly for structure: if an EA fails to converge in time, maybe just use a constructive output it had initially.

Cached Presets: Maintain a small library of pre-validated scenarios. If generation truly fails (perhaps extremely rare if our algorithms are robust), we use a cached scenario. We ensure that it’s injected with a fresh random tweak so it’s not identical to a known scenario (maybe shuffle some symmetric elements). This ensures the match can go on. It’s essentially a safety net.

Graceful Degradation: In scoring, if we only got a scenario that is fair but maybe not very “watchable” due to time, it’s better than cancelling match. So we allow using any valid scenario if need be, and just note its properties. We prefer that over waiting indefinitely for a perfect one.

Parallel Generation: If the hardware has multiple cores, do structure+fill for multiple seeds in parallel threads to use time efficiently (especially since a lot of PCG tasks are CPU-bound but parallelizable). This can ensure within ~same wall-clock we get several options. Careful with determinism: ensure separate RNG streams per thread, or better, do sequentially to avoid subtle nondeterminism (or if using parallel, fix scheduling or use only functional pure methods to generate).

Profiling worst-case: We will benchmark scenarios of maximum size/complexity allowed (maybe largest map, max items) to ensure even they meet the budget. If not, we scale down or optimize algorithms.

Profiling Guidance: We integrate profiling in development: instrument the generator to record time spent in each stage (structure, fill, validate segments). Over many runs, we accumulate stats (average, 95th percentile, max). This helps identify bottlenecks. For example, if we find fill occasionally takes 400 ms due to backtracking, we might refine the algorithm or add more constraints to reduce search.
We also profile memory usage – PCG shouldn’t use more than, say, a few hundred MB worst-case. Most algorithms here are lightweight, but if we simulate a large state space, that could use memory.

Benchmark Cases: We create a set of test seeds: some random, some pathological (if known). For instance, a seed that makes WFC nearly unsolvable (if exists) to see how it behaves. Or a structure that creates a very branching maze (to test solver). We run them offline and measure to ensure none exceed time budget significantly. Those that do may inform adding explicit checks: e.g. detect if WFC is thrashing and break early to retry.

Candidate Count vs. Time: Another angle – how many candidates is optimum? We find diminishing returns after a point. We might benchmark that generating 5 and picking best yields maybe 10% better score than generating 1, but generating 10 yields only 2% more than 5. We’ll choose accordingly. Perhaps dynamic: if first scenario generated is already excellent (above some threshold), we could just use it and skip generating more to save time (call this an early stopping if quality is high).

Engine Overhead: Running the actual match engine for simulation in validation might be heavy if done naively. We might consider building a simplified game model for the validator (to simulate approximate outcomes faster). That itself should be tested against full game for accuracy.

Real-Time Constraints: If in a live match a new scenario must load between rounds, and players expect minimal downtime, we ensure any generation is pre-computed or extremely fast. For example, if a game has rounds of 2 minutes and between each round we want a fresh map in <5 seconds, our pipeline fits in 2s which is fine. If even that is too slow, we would pre-generate maps ahead of time or lower N. But currently, a couple seconds at match start is acceptable; players often have load time or a “map preview” stage anyway.

Scalability: For tournament scheduling, if multiple matches run in parallel on one server, generation overhead multiplies. We must ensure concurrency doesn’t bog it down – likely generation tasks are low CPU enough that handling a dozen concurrently is fine. If not, consider staggering or dedicating some compute for PCG tasks.

We will continuously benchmark in staging environments: e.g. run 1000 random generation cycles and see if any approach timeouts. This statistical profiling ensures our 99.9th percentile is still under, say, 5 seconds. If not, adjust algorithms or reduce complexity until it is.

Performance vs Quality Trade-off: We should note that spending more time could yield better scenarios. For instance, an evolutionary algorithm given 5 seconds might produce a more balanced map than our 0.5s approach. In future, if we want maximum quality, we could allow longer generation offline or between tournament days. But for live, we stick to the budget. We might experiment with offline “evolutionary tuning” of generator parameters using more time, then use the learned parameters in the live generator (so the heavy lifting is done offline).

Resource Budgets: Also set budgets for things like:

number of candidates considered (to avoid combinatorial blow-up),

maximum iterations in WFC or solver (after which fail out),

memory (don’t generate extremely large maps beyond spec),

and even consider frame budget if generation must happen in an existing game engine tick.

By planning these budgets, we ensure the system is predictable and reliable, not causing delays or crashes. This kind of profiling and budget enforcement is a best practice echoed in game dev where PCG is used during runtime (ensuring no huge hitches).

Testing Strategy

To guarantee the PCG system’s correctness, balance, and ongoing quality, we implement a comprehensive testing infrastructure:

1. Property-Based Testing: We treat the generator as a function that should satisfy certain properties for all (or most) inputs (seeds). Using property-based testing frameworks (like QuickCheck/Hypothesis style), we will automatically generate random seeds and verify invariants. For example, a property test might generate 100 random scenarios and assert: “for each scenario, ValidationReport.solvable is true and fairness_index >= 0.9”. If any seed violates that, it’s a bug in generator or too strict an invariant. We also test structural properties: e.g., ensure every generated map has at least one path between spawns (no completely disconnected spawns). This can catch logic errors early. Another example: ensure that the generator output is deterministic – we run it twice with same seed and diff the outputs (they must match exactly). We include extreme seeds like 0 or max 64-bit, etc., to test edge cases. These property tests run in CI to prevent regressions (if someone changes generation code, and now occasionally produces unsolvable map, a test fails).

We design tests for each ValidationReport field too: e.g., triviality measure should flag known trivial constructs. We might manually craft or stub a scenario to feed into the validator to see if it catches the issues (like a scenario file where one player immediately wins – does validator mark trivial? it should).

2. Regression Seed Bank: Over time, we will accumulate a set of interesting seeds:

Seeds that previously caused bugs (e.g., a seed that produced an unsolvable map due to a rare generator bug).

Seeds that produced borderline fairness or weird but valid maps.

Seeds from actual tournament games that were notable (exciting or problematic).
We maintain these in a repository. As we update the generator, we re-run it on all seeds in this bank and ensure that either it produces identical results (if the algorithm shouldn’t have changed that scenario), or if changes are intended (like improving fairness), then at least no new violations introduced. For deterministic output, a small change might produce different maps (since generation logic changed). So for those tests, we focus on outcomes: e.g., after a code change, does seed X now yield a map that still passes all validations and is not worse in metrics than before? We can even store the previous ValidationReport for each seed and compare new vs old. If a change unexpectedly drops fairness for a regression seed, that’s a red flag.

We also keep some “golden scenarios” manually inspected and ensure the generator still can produce them (or deliberately doesn’t produce ones that were undesirable anymore). Essentially, this is like unit tests but with full scenarios as units.

3. Generator Fuzzing: Beyond property-based, we can use fuzz testing where the fuzzer generates unusual inputs or simulates partial failures. However, our input is mainly a seed (which is just an integer) – fuzzing that is same as random testing. A more interesting fuzz could be to fuzz internal parameters: e.g., randomly enable/disable parts of generator pipeline to see if any combination causes a crash or hang. We might instrument the generation code with timeouts and use a fuzzer that tries to maximize coverage (maybe something like AFL but for our context might not apply directly, as it's not parsing external input aside from the seed).

Another fuzz angle: treat the game state as input to validator and fuzz game states to see if validator ever crashes or misestimates. This ensures the validator is robust even if given weird states.

4. Statistical Validation Tools: We will continuously validate that the distribution of scenarios remains as intended. This involves generating a large sample (say 10k scenarios) and computing aggregate statistics:

Distribution of fairness_index, distribution of branching factor, etc. Ideally, these cluster around desired values (e.g. fairness mostly 0.95–1.0). If we notice a drift or some bias (e.g. scenarios tend to favor Player 1 55% of time on average – not good), we adjust the generator.

We also use statistical tests to ensure variety: e.g., measure how often certain map features appear. If our PCG should be uniform but our analysis finds, say, that 20% of maps have a particular pattern and another pattern only 1%, that might indicate bias to fix (unless intentional).

Coverage testing: define features like “has water”, “has 3 choke points”, etc., and see if our random generation can produce all combinations over many runs. If some theoretically possible combination never shows up in 100k samples, maybe our generation logic inadvertently forbids it – check if that’s fine or a bug.

No catastrophic failures: we simulate matches with various agents on a big batch of maps to ensure none lead to trivial immediate wins or extremely long stalemates beyond design (if any do, those scenario classes might need filtering).

5. Performance Tests: We create automated benchmarks as part of tests that run the generation pipeline, say 1000 times, and ensure it stays within time limits on the target hardware. If performance regresses (like a commit makes generation 2x slower), that should flag if it threatens realtime use. We can have thresholds in CI that fail if generation time > X ms on average. Similarly memory – run under valgrind or similar to catch leaks or giant allocations.

6. Simulation-based Balancing Tests: As an extra, we might integrate a test where a simple AI plays both sides on a bunch of generated maps, and we verify that win rates are ~50%. If any scenario consistently yields, say, >70% wins for one side with symmetric play, it’s likely imbalanced. This acts as a high-level property test for fairness beyond static metrics. We could do this for a few representative agents (like a random agent, a greedy agent) to catch extreme imbalances.

7. Fuzzer for Adversarial Scenarios: We might use an adversarial generator fuzz in testing: try to generate scenarios that are worst-case for our agents to ensure they still handle it. This is more for agent testing than generation, but relevant: one can use adversarial environment generation (like PAIRED algorithms) in a test context to see if any scenario breaks our defined limits or violates assumptions. For instance, an adversarial search might try to create the most unfair map given our generator’s flexibility – if it finds one that passes our validation but is still unfair in simulation, then our metrics missed something. We then update metrics to catch that.

8. Continuous Tournament Simulation: For end-to-end testing, run a “mock tournament” regularly where multiple bot agents (could be scripted or previous competition AIs) play matches with generated scenarios. Monitor if any anomalies happen: e.g., any match ends instantly (possible trivial map), any match never ends (possible unsolvable or bug causing loop), or any agent crashes (maybe generator gave weird input to agent?). This full integration test ensures generator, game engine, agents interplay properly. We also gather stats from these matches automatically (like average scores, win times) to see if any metric drifts over software updates.

Tooling: We’ll build small CLI tools: e.g., scenario_generate --seed S to output a scenario file and its ValidationReport, so we can easily manually inspect certain seeds. Also a scenario_fuzzer that loops random seeds and prints any that fail validation or that hit slow performance. For automated tests, integration with CI so that if a test fails (like generator produced invalid scenario), the build fails and we fix before release.

Testing the Watchability scoring: This is more subjective, but we can test consistency: if we manually label some scenarios as “exciting” vs “dull”, does our score correlate? We might simulate a known scenario with lots of fighting and see that our swing metrics are indeed higher than a scenario known to be one-sided. Over time, if actual matches have audience feedback, we can adjust these metrics.

Randomness and Seeds in Testing: Note that our PCG is random; tests should be deterministic. For property-based, we use fixed seed for the test harness or explicitly list seeds to test (though tools often randomize but then log the random seed used so failures can be reproduced). We will ensure any random failing seed is captured so we can add it to regression bank. The determinism of generation with a given seed is helpful here – tests are repeatable.

Edge Cases: We test boundaries: smallest map, largest map, no obstacles vs max obstacles, etc. Also test unusual but possible rule configurations if scenario has variants. Ensure validator not dividing by zero etc. Possibly feed malformed inputs to validator to ensure it handles gracefully (though those shouldn’t occur if generator is correct, but belt-and-suspenders).

In summary, our testing strategy is extensive: from unit-level property tests to full system simulations. It ensures that each stage of the pipeline works as expected and that the whole delivers fair, fun scenarios reliably. Issues found in testing (like any bias or bug) will be fed back into improving algorithms. This continuous validation aligns with recommendations in PCG research to keep a human or automated check on generator outputs to avoid “PCG gone wrong” scenarios. Our toolkit of fuzzing and property tests will give us high confidence in the generator’s robustness before it ever faces real competitors.

References: Our approach builds upon recent research in procedural generation and competitive game design. Symmetry and fairness considerations follow from studies on game balance via PCG and multiobjective map generation for fairness. The commit-reveal protocol is informed by provably fair gaming systems. Watchability metrics draw from sports analytics where excitement is quantified through win probability swings and lead changes. The PAIRED framework for adversarial environment generation highlights how unpredictability and curriculum in scenarios can improve robustness, reinforcing our emphasis on diversity and anti-precomputation. Our safety checks and coach restrictions are aligned with eSports integrity standards, ensuring no repetition of spectator exploitation scandals. By integrating these lessons, our platform aims to deliver dynamic yet fair contests that are secure from exploitation and thrilling for spectators. The result is a deterministic, seed-fair, and adversary-resilient PCG system poised to support competitive AI matches that are as balanced as they are exciting.
